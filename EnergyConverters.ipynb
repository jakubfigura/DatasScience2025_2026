{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zv2z7XITsU1q"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "adelaide_df = pd.read_csv('energy/Adelaide_Data.csv')\n",
    "perth_df = _df = pd.read_csv('energy/Perth_Data.csv')\n",
    "sydney_df = pd.read_csv('energy/Sydney_Data.csv')\n",
    "tasmania_df = _df = pd.read_csv('energy/Tasmania_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EhOYqyYQteiT",
    "outputId": "97bb89b0-9b4e-45c1-98dd-dacc1a74ef6c"
   },
   "outputs": [],
   "source": [
    "#check if dataset has any wrong values\n",
    "print(\"--- Adelaide DataFrame Info ---\")\n",
    "adelaide_df.info()\n",
    "print(\"\\n--- Adelaide DataFrame Null Counts ---\")\n",
    "print(adelaide_df.isnull().sum())\n",
    "print(f\"\\n--- Adelaide DataFrame Duplicate Rows: {adelaide_df.duplicated().sum()} ---\")\n",
    "print(\"\\n--- Adelaide DataFrame Descriptive Statistics ---\")\n",
    "display(adelaide_df.describe())\n",
    "\n",
    "print(\"\\n\\n--- Perth DataFrame Info ---\")\n",
    "perth_df.info()\n",
    "print(\"\\n--- Perth DataFrame Null Counts ---\")\n",
    "print(perth_df.isnull().sum())\n",
    "print(f\"\\n--- Perth DataFrame Duplicate Rows: {perth_df.duplicated().sum()} ---\")\n",
    "print(\"\\n--- Perth DataFrame Descriptive Statistics ---\")\n",
    "display(perth_df.describe())\n",
    "\n",
    "print(\"\\n\\n--- Sydney DataFrame Info ---\")\n",
    "sydney_df.info()\n",
    "print(\"\\n--- Sydney DataFrame Null Counts ---\")\n",
    "print(sydney_df.isnull().sum())\n",
    "print(f\"\\n--- Sydney DataFrame Duplicate Rows: {sydney_df.duplicated().sum()} ---\")\n",
    "print(\"\\n--- Sydney DataFrame Descriptive Statistics ---\")\n",
    "display(sydney_df.describe())\n",
    "\n",
    "print(\"\\n\\n--- Tasmania DataFrame Info ---\")\n",
    "tasmania_df.info()\n",
    "print(\"\\n--- Tasmania DataFrame Null Counts ---\")\n",
    "print(tasmania_df.isnull().sum())\n",
    "print(f\"\\n--- Tasmania DataFrame Duplicate Rows: {tasmania_df.duplicated().sum()} ---\")\n",
    "print(\"\\n--- Tasmania DataFrame Descriptive Statistics ---\")\n",
    "display(tasmania_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z54dUW33xBnj"
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# cols 1-16 represent X coordinates\n",
    "# cols 17-32 represent Y coordinates\n",
    "# cols 33 - 48 represent power of single containers\n",
    "# col 49 represents sum of power in the farm\n",
    "# We want to calculate Powerall depending on X, Y coordinates\n",
    "\n",
    "X_cols = [f\"X{i}\" for i in range(1, 17)]\n",
    "Y_cols = [f\"Y{i}\" for i in range(1, 17)]\n",
    "powerall_col = [\"Powerall\"]\n",
    "\n",
    "def split_data(subset, powerall):\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(subset, powerall, test_size=0.4, random_state=44)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=44)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def prepare_subsets(dataframe):\n",
    "    subset = dataframe[X_cols + Y_cols]\n",
    "    powerall = dataframe[powerall_col]\n",
    "    split_data(subset, powerall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_vs_actual(y_test, y_predict, name, r2, rmse, mape, show_plot=False):\n",
    "    \"\"\"\n",
    "    Generates a scatter plot of predicted vs. actual values.\n",
    "    \n",
    "    y_test: Actual values\n",
    "    y_predict: Predicted values\n",
    "    title: Title for the plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_test, y_predict, alpha=0.5, label='Predicted vs Actual')\n",
    "   \n",
    "    # Ideal line (y=x)\n",
    "    min_val = min(y_test.min().iloc[0], y_predict.min())\n",
    "    max_val = max(y_test.max().iloc[0], y_predict.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, label='Ideal Fit (y=x)')\n",
    "   \n",
    "    plt.xlabel(\"Actual Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.title(f\"Predicted vs Actual for {name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Display R2, RMSE, and MAPE values on the plot\n",
    "    textstr = f\"R2: {r2:.4f}\\nRMSE: {rmse:.2f}\\nMAPE: {mape:.2f}%\"\n",
    "    plt.gca().text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=10,\n",
    "                   verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "    \n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    plots_dir = 'plots/predicted_vs_actualy'\n",
    "    if not os.path.exists(plots_dir):\n",
    "        os.makedirs(plots_dir)\n",
    "   \n",
    "    filename = name.replace(' ', '_') + '.png'\n",
    "    filepath = os.path.join(plots_dir, filename)\n",
    "   \n",
    "    plt.savefig(filepath)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_residuals(y_test, y_predict, name,  r2, rmse, mape, show_plot=False):\n",
    "    \"\"\"\n",
    "    Generates a scatter plot of residuals vs. predicted values.\n",
    "       \n",
    "    y_test: Actual values\n",
    "    y_predict: Predicted values\n",
    "    title: Title for the plot\n",
    "    name: Name for saving the file\n",
    "    \"\"\"\n",
    "    residuals = y_test.values.flatten() - y_predict.flatten()\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_predict, residuals, alpha=0.5)\n",
    "        \n",
    "    # Horizontal line at y=0\n",
    "    plt.axhline(y=0, color='k', linestyle='--', lw=2)\n",
    "        \n",
    "    plt.xlabel(\"Predicted Values\")\n",
    "    plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
    "    plt.title(f\"Residuals vs Predicted Values for {name}\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Display R2, RMSE, and MAPE values on the plot\n",
    "    textstr = f\"R2: {r2:.4f}\\nRMSE: {rmse:.2f}\\nMAPE: {mape:.2f}%\"\n",
    "    plt.gca().text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=10,\n",
    "                   verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "    \n",
    "        \n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "        \n",
    "    plots_dir = 'plots/residuals'\n",
    "    if not os.path.exists(plots_dir):\n",
    "        os.makedirs(plots_dir)\n",
    "        \n",
    "    filename = name.replace(' ', '_') + '_residuals.png'\n",
    "    filepath = os.path.join(plots_dir, filename)\n",
    "        \n",
    "    plt.savefig(filepath)\n",
    "    plt.close()\n",
    "\n",
    "def plot_residuals_distribution(y_test, y_predict, name,  r2, rmse, mape, show_plot=False):\n",
    "    \"\"\"\n",
    "    Generates a histogram of the residuals to show their distribution.\n",
    "        \n",
    "    y_test: Actual values\n",
    "    y_predict: Predicted values\n",
    "    title: Title for the plot\n",
    "    name: Name for saving the file\n",
    "    \"\"\"\n",
    "    residuals = y_test.values.flatten() - y_predict.flatten()\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "        \n",
    "    plt.hist(residuals, bins=50, density=True, alpha=0.6, color='g', label='Residuals Histogram')\n",
    "        \n",
    "    # Fit a normal distribution to the data\n",
    "    mu, std = stats.norm.fit(residuals)\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = stats.norm.pdf(x, mu, std)\n",
    "    plt.plot(x, p, 'k', linewidth=2, label='Normal Distribution Fit')\n",
    "        \n",
    "    plt.axvline(x=0, color='r', linestyle='--', lw=2, label='Zero Residuals')\n",
    "\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(f\"Residuals Distribution for {name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Display R2, RMSE, and MAPE values on the plot\n",
    "    textstr = f\"R2: {r2:.4f}\\nRMSE: {rmse:.2f}\\nMAPE: {mape:.2f}%\"\n",
    "    plt.gca().text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=10,\n",
    "                   verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "        \n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "            \n",
    "    plots_dir = 'plots/residuals_distribution'\n",
    "    if not os.path.exists(plots_dir):\n",
    "        os.makedirs(plots_dir)\n",
    "            \n",
    "    filename = name.replace(' ', '_') + '_residuals_dist.png'\n",
    "    filepath = os.path.join(plots_dir, filename)\n",
    "            \n",
    "    plt.savefig(filepath)\n",
    "    plt.close()\n",
    "\n",
    "def make_plots(y_test, y_predict, name, r2, rmse, mape):\n",
    "    plot_predicted_vs_actual(y_test, y_predict, name, r2, rmse, mape)\n",
    "    plot_residuals(y_test, y_predict, name, r2, rmse, mape)\n",
    "    plot_residuals_distribution(y_test, y_predict, name, r2, rmse, mape)\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_poly_regression(dataframe, degree, name):\n",
    "    \n",
    "   X_train, X_val, X_test, y_train, y_val, y_test = prepare_subsets(dataframe)\n",
    "\n",
    "   poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "   poly = poly_features.fit_transform(subset)\n",
    "\n",
    "   X_train, X_test, y_train, y_test = train_test_split(poly, powerall, test_size=0.2, random_state=44)\n",
    "\n",
    "   model = LinearRegression()\n",
    "   \n",
    "   model.fit(X_train, y_train)\n",
    "   y_predict = model.predict(X_test)\n",
    "   r2 = model.score(X_test, y_test)\n",
    "   rmse = np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "   mape = mean_absolute_percentage_error(y_test, y_predict) * 100\n",
    "   print(f\"{name}\\nScore: {r2}\")\n",
    "   print(f\"RMSE: {np.sqrt(rmse):.3f}\")\n",
    "   print(f\"MAPE: {mape:.3f}%\\n\")\n",
    "\n",
    "   results[name] = {\n",
    "      \"score\": r2,\n",
    "      \"rmse\": rmse,\n",
    "      \"mape\": mape\n",
    "   }\n",
    "\n",
    "   make_plots(y_test, y_predict, name, r2, rmse, mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_poly_regression(adelaide_df, 1, \"Adelaide - Linear Regression\")\n",
    "train_poly_regression(perth_df, 1, \"Perth - Linear Regression\")\n",
    "train_poly_regression(sydney_df, 1, \"Sydney - Linear Regression\")\n",
    "train_poly_regression(tasmania_df, 1, \"Tasmania - Linear Regression\")\n",
    "\n",
    "train_poly_regression(adelaide_df, 2, \"Adelaide - Polynomial Regression Degree 2\")\n",
    "train_poly_regression(perth_df, 2, \"Perth - Polynomial Regression Degree 2\")\n",
    "train_poly_regression(sydney_df, 2, \"Sydney - Polynomial Regression Degree 2\")\n",
    "train_poly_regression(tasmania_df, 2, \"Tasmania - Polynomial Regression Degree 2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def train_xgb_model(dataframe, name):\n",
    "\n",
    "    subset = dataframe[X_cols + Y_cols]\n",
    "    powerall = dataframe[powerall_col]\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(subset, powerall, test_size=0.2, random_state=44)\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor (objective=\"reg:squarederror\", n_estimators = 1500, random_state=44, subsample= 0.8, learning_rate= 0.07)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    y_predict = xgb_model.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, y_predict)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_predict) * 100\n",
    "\n",
    "    print(f\"{name}\\nScore: {r2}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"MAPE: {mape:.3f}%\\n\")\n",
    "\n",
    "    results[name] = {\n",
    "        \"score\": r2,\n",
    "        \"rmse\": rmse,\n",
    "        \"mape\": mape\n",
    "        }\n",
    "    \n",
    "    make_plots(y_test, y_predict, name, r2, rmse, mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgb_model(adelaide_df, \"Adelaide - XGBoost Regressor\")\n",
    "train_xgb_model(perth_df, \"Perth - XGBoost Regressor\")\n",
    "train_xgb_model(sydney_df, \"Sydney - XGBoost Regressor\")\n",
    "train_xgb_model(tasmania_df, \"Tasmania - XGBoost Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "def train_lgb_model(dataframe, name):\n",
    "\n",
    "    subset = dataframe[X_cols + Y_cols]\n",
    "    powerall = dataframe[powerall_col]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(subset, powerall, test_size=0.2, random_state=44)\n",
    "\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        objective=\"regression\", \n",
    "        n_estimators=1500, \n",
    "        random_state=44, \n",
    "        subsample=0.8, \n",
    "        learning_rate=0.07\n",
    "    )\n",
    "    \n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    y_predict = lgb_model.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, y_predict)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_predict) * 100\n",
    "\n",
    "    print(f\"{name}\\nScore: {r2}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"MAPE: {mape:.3f}%\\n\")\n",
    "\n",
    "    results[name] = {\n",
    "        \"score\": r2,\n",
    "        \"rmse\": rmse,\n",
    "        \"mape\": mape\n",
    "    }\n",
    "\n",
    "    make_plots(y_test, y_predict, name, r2, rmse, mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lgb_model(adelaide_df, \"Adelaide - LightGBM Regressor\")\n",
    "train_lgb_model(perth_df, \"Perth - LightGBM Regressor\")\n",
    "train_lgb_model(sydney_df, \"Sydney - LightGBM Regressor\")\n",
    "train_lgb_model(tasmania_df, \"Tasmania - LightGBM Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "\n",
    "def train_catboost_model(dataframe, name):\n",
    "    X_cols = [f\"X{i}\" for i in range(1, 17)]\n",
    "    Y_cols = [f\"Y{i}\" for i in range(1, 17)]\n",
    "    powerall_col = [\"Powerall\"]\n",
    "    subset = dataframe[X_cols + Y_cols]\n",
    "    powerall = dataframe[powerall_col]\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(subset, powerall, test_size=0.4, random_state=44)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=44)\n",
    "\n",
    "    catboost_model = cb.CatBoostRegressor(\n",
    "        iterations=3000,\n",
    "        learning_rate=0.05,\n",
    "        depth=7,\n",
    "        l2_leaf_reg=3,\n",
    "        random_seed=44,\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "    \n",
    "    catboost_model.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        eval_set=(X_val, y_val),\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    y_predict = catboost_model.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, y_predict)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_predict) * 100\n",
    "\n",
    "    print(f\"{name}\\nScore: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"MAPE: {mape:.3f}%\\n\")\n",
    "\n",
    "    results[name] = {\n",
    "        \"score\": r2,\n",
    "        \"rmse\": rmse,\n",
    "        \"mape\": mape\n",
    "    }\n",
    "\n",
    "    make_plots(y_test, y_predict, name, r2, rmse, mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_catboost_model(adelaide_df, \"Adelaide - CatBoost Regressor\")\n",
    "train_catboost_model(perth_df, \"Perth - CatBoost Regressor\")\n",
    "train_catboost_model(sydney_df, \"Sydney - CatBoost Regressor\")\n",
    "train_catboost_model(tasmania_df, \"Tasmania - CatBoost Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble_model(dataframe, name):\n",
    "    X_cols = [f\"X{i}\" for i in range(1, 17)]\n",
    "    Y_cols = [f\"Y{i}\" for i in range(1, 17)]\n",
    "    powerall_col = [\"Powerall\"]\n",
    "    subset = dataframe[X_cols + Y_cols]\n",
    "    powerall = dataframe[powerall_col]\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(subset, powerall, test_size=0.4, random_state=44)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=44)\n",
    "\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=3000, \n",
    "                                  learning_rate=0.05, max_depth=7, subsample=0.8, \n",
    "                                  random_state=44, colsample_bytree=0.8)\n",
    "    \n",
    "    lgb_model = lgb.LGBMRegressor(objective=\"regression\", n_estimators=3000, \n",
    "                                   learning_rate=0.05, max_depth=7, subsample=0.8, \n",
    "                                   random_state=44, colsample_bytree=0.8, verbose=-1)\n",
    "    \n",
    "    catboost_model = cb.CatBoostRegressor(iterations=3000, learning_rate=0.05, \n",
    "                                          depth=7, random_seed=44, verbose=False)\n",
    "\n",
    " \n",
    "    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                  callbacks=[lgb.early_stopping(stopping_rounds=100)])\n",
    "    catboost_model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)\n",
    "\n",
    "\n",
    "    pred_xgb = xgb_model.predict(X_test)\n",
    "    pred_lgb = lgb_model.predict(X_test)\n",
    "    pred_cat = catboost_model.predict(X_test)\n",
    "\n",
    "\n",
    "    y_predict = 0.36 * pred_xgb + 0.36 * pred_lgb + 0.28 * pred_cat\n",
    "\n",
    "    r2 = r2_score(y_test, y_predict)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_predict) * 100\n",
    "\n",
    "    print(f\"{name}\\nScore: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"MAPE: {mape:.3f}%\\n\")\n",
    "\n",
    "    results[name] = {\n",
    "        \"score\": r2,\n",
    "        \"rmse\": rmse,\n",
    "        \"mape\": mape\n",
    "    }\n",
    "\n",
    "    make_plots(y_test, y_predict, name, r2, rmse, mape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble_model(adelaide_df, \"Adelaide - Ensemble (XGB+LGB+Cat)\")\n",
    "train_ensemble_model(perth_df, \"Perth - Ensemble (XGB+LGB+Cat)\")\n",
    "train_ensemble_model(sydney_df, \"Sydney - Ensemble (XGB+LGB+Cat)\")\n",
    "train_ensemble_model(tasmania_df, \"Tasmania - Ensemble (XGB+LGB+Cat)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial, X_train, y_train, X_val, y_val):\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"n_estimators\": 2000,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"random_state\": 44,\n",
    "        \"n_jobs\": -1\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBRegressor(\n",
    "        **params,\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr(\"best_iteration\", model.best_iteration)\n",
    "    \n",
    "    \n",
    "    y_predict = model.predict(X_val)\n",
    "    \n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_predict))\n",
    "    return rmse\n",
    "\n",
    "def train_xgb_with_optuna(dataframe, name, n_trials=50):\n",
    "    \n",
    "    print(f\"--- Optuna tuning for: {name} ---\")\n",
    "    \n",
    "    X_cols = [f\"X{i}\" for i in range(1, 17)]\n",
    "    Y_cols = [f\"Y{i}\" for i in range(1, 17)]\n",
    "    powerall_col = [\"Powerall\"]\n",
    "    subset = dataframe[X_cols + Y_cols]\n",
    "    powerall = dataframe[powerall_col]\n",
    "\n",
    "\n",
    "    # 60% train, 40% temp\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(subset, powerall, test_size=0.4, random_state=44)\n",
    "    # 50% of temp -> validation\n",
    "    # 50% of temp -> test\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=44)\n",
    "\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, X_train, y_train, X_val, y_val), \n",
    "        n_trials=n_trials\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- Tuning for: {name} ended ---\")\n",
    "    print(f\"The best iteration (Validation RMSE): {study.best_value:.3f}\")\n",
    "    print(\"The best hiperparameters:\")\n",
    "    print(study.best_params)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_iteration = study.best_trial.user_attrs[\"best_iteration\"]\n",
    "    \n",
    "    # Combine train (60%) and validation (20%) sets for final training\n",
    "    X_train_full = pd.concat([X_train, X_val])\n",
    "    y_train_full = pd.concat([y_train, y_val])\n",
    "    \n",
    "    final_model = xgb.XGBRegressor(\n",
    "        **best_params,\n",
    "        n_estimators=best_iteration,\n",
    "        random_state=44,\n",
    "        n_jobs=4\n",
    "    )\n",
    "    \n",
    "    final_model.fit(X_train_full, y_train_full)\n",
    "    \n",
    "    y_predict = final_model.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_predict)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_predict) * 100\n",
    "\n",
    "    print(f\"\\n--- Results for {name} ---\")\n",
    "    print(f\"Score (R2): {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"MAPE: {mape:.3f}%\\\\n\")\n",
    "    results[f\"{name}_Optuna_R2\"] = {\"score\": r2, \"rmse\": rmse, \"mape\": mape}\n",
    "    make_plots(y_test, y_predict, name + \" - Optuna Tuning\", r2, rmse, mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgb_with_optuna(adelaide_df, \"Adelaide - XGBoost with Optuna HPT\", n_trials=50)\n",
    "train_xgb_with_optuna(perth_df, \"Perth - XGBoost with Optuna HPT\", n_trials=50)\n",
    "train_xgb_with_optuna(sydney_df, \"Sydney - XGBoost with Optuna HPT\", n_trials=50)\n",
    "train_xgb_with_optuna(tasmania_df, \"Tasmania - XGBoost with Optuna HPT\", n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgb(trial, X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"regression_l2\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"n_estimators\": 2000,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 150),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"random_state\": 44,\n",
    "        \"n_jobs\": -1,\n",
    "        \"verbose\": -1\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"rmse\",\n",
    "        callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr(\"best_iteration\", model.best_iteration_)\n",
    "    \n",
    "    y_predict = model.predict(X_val)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_predict))\n",
    "    return rmse\n",
    "\n",
    "def train_lgb_with_optuna(dataframe, name, n_trials=50):\n",
    "    \n",
    "    print(f\"--- Optuna tuning for: {name} ---\")\n",
    "    \n",
    "    X_cols = [f\"X{i}\" for i in range(1, 17)]\n",
    "    Y_cols = [f\"Y{i}\" for i in range(1, 17)]\n",
    "    powerall_col = [\"Powerall\"]\n",
    "    subset = dataframe[X_cols + Y_cols]\n",
    "    powerall = dataframe[powerall_col]\n",
    "\n",
    "    # 60% train, 40% temp\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(subset, powerall, test_size=0.4, random_state=44)\n",
    "    # 50% of temp -> validation\n",
    "    # 50% of temp -> test\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=44)\n",
    "\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(\n",
    "        lambda trial: objective_lgb(trial, X_train, y_train, X_val, y_val), \n",
    "        n_trials=n_trials\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- Tuning for: {name} ended ---\")\n",
    "    print(f\"The best iteration (Validation RMSE): {study.best_value:.3f}\")\n",
    "    print(\"The best hiperparameters:\")\n",
    "    print(study.best_params)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_iteration = study.best_trial.user_attrs[\"best_iteration\"]\n",
    "    \n",
    "    # Combine train (60%) and validation (20%) sets for final training\n",
    "    X_train_full = pd.concat([X_train, X_val])\n",
    "    y_train_full = pd.concat([y_train, y_val])\n",
    "    \n",
    "    final_model = lgb.LGBMRegressor(\n",
    "        **best_params,\n",
    "        n_estimators=best_iteration,\n",
    "        random_state=44,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    final_model.fit(X_train_full, y_train_full)\n",
    "    \n",
    "    y_predict = final_model.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_predict)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_predict) * 100\n",
    "\n",
    "    print(f\"\\n--- Results for {name} ---\")\n",
    "    print(f\"Score (R2): {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"MAPE: {mape:.3f}%\\\\n\")\n",
    "    results[f\"{name}_Optuna_LGB\"] = {\"score\": r2, \"rmse\": rmse, \"mape\": mape}\n",
    "    make_plots(y_test, y_predict, name + \" - Optuna Tuning\", r2, rmse, mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lgb_with_optuna(adelaide_df, \"Adelaide - LightGBM with Optuna HPT\", n_trials=50)\n",
    "train_lgb_with_optuna(perth_df, \"Perth - LightGBM with Optuna HPT\", n_trials=50)\n",
    "train_lgb_with_optuna(sydney_df, \"Sydney - LightGBM with Optuna HPT\", n_trials=50)\n",
    "train_lgb_with_optuna(tasmania_df, \"Tasmania - LightGBM with Optuna HPT\", n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "\n",
    "def objective_cb(trial, X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"RMSE\",\n",
    "        \"eval_metric\": \"RMSE\",\n",
    "        \"iterations\": 2000,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 3, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-8, 10.0, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 1e-8, 10.0, log=True),\n",
    "        \"random_seed\": 44,\n",
    "        \"thread_count\": -1,\n",
    "        \"verbose\": False\n",
    "    }\n",
    "\n",
    "    model = cb.CatBoostRegressor(\n",
    "        **params,\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=(X_val, y_val),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr(\"best_iteration\", model.get_best_iteration())\n",
    "    \n",
    "    y_predict = model.predict(X_val)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_predict))\n",
    "    return rmse\n",
    "\n",
    "def train_cb_with_optuna(dataframe, name, n_trials=50):\n",
    "    \n",
    "    print(f\"--- Optuna tuning for: {name} ---\")\n",
    "    \n",
    "    X_cols = [f\"X{i}\" for i in range(1, 17)]\n",
    "    Y_cols = [f\"Y{i}\" for i in range(1, 17)]\n",
    "    powerall_col = [\"Powerall\"]\n",
    "    subset = dataframe[X_cols + Y_cols]\n",
    "    powerall = dataframe[powerall_col]\n",
    "\n",
    "    # 60% train, 40% temp\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(subset, powerall, test_size=0.4, random_state=44)\n",
    "    # 50% of temp -> validation\n",
    "    # 50% of temp -> test\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=44)\n",
    "\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(\n",
    "        lambda trial: objective_cb(trial, X_train, y_train, X_val, y_val), \n",
    "        n_trials=n_trials\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- Tuning for: {name} ended ---\")\n",
    "    print(f\"The best iteration (Validation RMSE): {study.best_value:.3f}\")\n",
    "    print(\"The best hiperparameters:\")\n",
    "    print(study.best_params)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_iteration = study.best_trial.user_attrs[\"best_iteration\"]\n",
    "    \n",
    "    # Combine train (60%) and validation (20%) sets for final training\n",
    "    X_train_full = pd.concat([X_train, X_val])\n",
    "    y_train_full = pd.concat([y_train, y_val])\n",
    "    \n",
    "    final_model = cb.CatBoostRegressor(\n",
    "        **best_params,\n",
    "        iterations=best_iteration,\n",
    "        random_seed=44,\n",
    "        thread_count=-1,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    final_model.fit(X_train_full, y_train_full, verbose=False)\n",
    "    \n",
    "    y_predict = final_model.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_predict)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_predict) * 100\n",
    "\n",
    "    print(f\"\\n--- Results for {name} ---\")\n",
    "    print(f\"Score (R2): {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"MAPE: {mape:.3f}%\\\\n\")\n",
    "    results[f\"{name}_Optuna_CB\"] = {\"score\": r2, \"rmse\": rmse, \"mape\": mape}\n",
    "    make_plots(y_test, y_predict, name + \" - Optuna Tuning\", r2, rmse, mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cb_with_optuna(adelaide_df, \"Adelaide - CatBoost with Optuna HPT\", n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cb_with_optuna(perth_df, \"Perth - CatBoost with Optuna HPT\", n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cb_with_optuna(sydney_df, \"Sydney - CatBoost with Optuna HPT\", n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_cb_with_optuna(tasmania_df, \"Tasmania - CatBoost with Optuna HPT\", n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
