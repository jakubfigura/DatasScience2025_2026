{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zv2z7XITsU1q"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "# Create column names: X1-X16 for first 16 cols, Y1-Y16 for next 16 cols, power columns for rest\n",
    "column_names = [f\"X{i}\" for i in range(1, 17)] + [f\"Y{i}\" for i in range(1, 17)] + [f\"Power{i}\" for i in range(1, 17)] + [\"Powerall\"]\n",
    "\n",
    "adelaide_df = pd.read_csv('energy/Adelaide_Data.csv', header=None, names=column_names)\n",
    "perth_df  = pd.read_csv('energy/Perth_Data.csv', header=None, names=column_names)\n",
    "sydney_df = pd.read_csv('energy/Sydney_Data.csv', header=None, names=column_names)\n",
    "tasmania_df = pd.read_csv('energy/Tasmania_Data.csv', header=None, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EhOYqyYQteiT",
    "outputId": "97bb89b0-9b4e-45c1-98dd-dacc1a74ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Adelaide DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 71999 entries, 0 to 71998\n",
      "Data columns (total 49 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   X1        71999 non-null  float64\n",
      " 1   X2        71999 non-null  float64\n",
      " 2   X3        71999 non-null  float64\n",
      " 3   X4        71999 non-null  float64\n",
      " 4   X5        71999 non-null  float64\n",
      " 5   X6        71999 non-null  float64\n",
      " 6   X7        71999 non-null  float64\n",
      " 7   X8        71999 non-null  float64\n",
      " 8   X9        71999 non-null  float64\n",
      " 9   X10       71999 non-null  float64\n",
      " 10  X11       71999 non-null  float64\n",
      " 11  X12       71999 non-null  float64\n",
      " 12  X13       71999 non-null  float64\n",
      " 13  X14       71999 non-null  float64\n",
      " 14  X15       71999 non-null  float64\n",
      " 15  X16       71999 non-null  float64\n",
      " 16  Y1        71999 non-null  float64\n",
      " 17  Y2        71999 non-null  float64\n",
      " 18  Y3        71999 non-null  float64\n",
      " 19  Y4        71999 non-null  float64\n",
      " 20  Y5        71999 non-null  float64\n",
      " 21  Y6        71999 non-null  float64\n",
      " 22  Y7        71999 non-null  float64\n",
      " 23  Y8        71999 non-null  float64\n",
      " 24  Y9        71999 non-null  float64\n",
      " 25  Y10       71999 non-null  float64\n",
      " 26  Y11       71999 non-null  float64\n",
      " 27  Y12       71999 non-null  float64\n",
      " 28  Y13       71999 non-null  float64\n",
      " 29  Y14       71999 non-null  float64\n",
      " 30  Y15       71999 non-null  float64\n",
      " 31  Y16       71999 non-null  float64\n",
      " 32  Power1    71999 non-null  float64\n",
      " 33  Power2    71999 non-null  float64\n",
      " 34  Power3    71999 non-null  float64\n",
      " 35  Power4    71999 non-null  float64\n",
      " 36  Power5    71999 non-null  float64\n",
      " 37  Power6    71999 non-null  float64\n",
      " 38  Power7    71999 non-null  float64\n",
      " 39  Power8    71999 non-null  float64\n",
      " 40  Power9    71999 non-null  float64\n",
      " 41  Power10   71999 non-null  float64\n",
      " 42  Power11   71999 non-null  float64\n",
      " 43  Power12   71999 non-null  float64\n",
      " 44  Power13   71999 non-null  float64\n",
      " 45  Power14   71999 non-null  float64\n",
      " 46  Power15   71999 non-null  float64\n",
      " 47  Power16   71999 non-null  float64\n",
      " 48  Powerall  71999 non-null  float64\n",
      "dtypes: float64(49)\n",
      "memory usage: 26.9 MB\n",
      "\n",
      "--- Adelaide DataFrame Null Counts ---\n",
      "X1          0\n",
      "X2          0\n",
      "X3          0\n",
      "X4          0\n",
      "X5          0\n",
      "X6          0\n",
      "X7          0\n",
      "X8          0\n",
      "X9          0\n",
      "X10         0\n",
      "X11         0\n",
      "X12         0\n",
      "X13         0\n",
      "X14         0\n",
      "X15         0\n",
      "X16         0\n",
      "Y1          0\n",
      "Y2          0\n",
      "Y3          0\n",
      "Y4          0\n",
      "Y5          0\n",
      "Y6          0\n",
      "Y7          0\n",
      "Y8          0\n",
      "Y9          0\n",
      "Y10         0\n",
      "Y11         0\n",
      "Y12         0\n",
      "Y13         0\n",
      "Y14         0\n",
      "Y15         0\n",
      "Y16         0\n",
      "Power1      0\n",
      "Power2      0\n",
      "Power3      0\n",
      "Power4      0\n",
      "Power5      0\n",
      "Power6      0\n",
      "Power7      0\n",
      "Power8      0\n",
      "Power9      0\n",
      "Power10     0\n",
      "Power11     0\n",
      "Power12     0\n",
      "Power13     0\n",
      "Power14     0\n",
      "Power15     0\n",
      "Power16     0\n",
      "Powerall    0\n",
      "dtype: int64\n",
      "\n",
      "--- Adelaide DataFrame Duplicate Rows: 0 ---\n",
      "\n",
      "--- Adelaide DataFrame Descriptive Statistics ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>Power8</th>\n",
       "      <th>Power9</th>\n",
       "      <th>Power10</th>\n",
       "      <th>Power11</th>\n",
       "      <th>Power12</th>\n",
       "      <th>Power13</th>\n",
       "      <th>Power14</th>\n",
       "      <th>Power15</th>\n",
       "      <th>Power16</th>\n",
       "      <th>Powerall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>71999.000000</td>\n",
       "      <td>71999.000000</td>\n",
       "      <td>71999.000000</td>\n",
       "      <td>71999.000000</td>\n",
       "      <td>71999.000000</td>\n",
       "      <td>71999.000000</td>\n",
       "      <td>71999.000000</td>\n",
       "      <td>71999.000000</td>\n",
       "      <td>71999.000000</td>\n",
       "      <td>71999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>71999.000000</td>\n",
       "      <td>71999.000000</td>\n",
       "      <td>71999.000000</td>\n",
       "      <td>71999.000000</td>\n",
       "      <td>71999.000000</td>\n",
       "      <td>71999.000000</td>\n",
       "      <td>71999.000000</td>\n",
       "      <td>71999.000000</td>\n",
       "      <td>71999.000000</td>\n",
       "      <td>7.199900e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>281.278924</td>\n",
       "      <td>279.316030</td>\n",
       "      <td>294.136617</td>\n",
       "      <td>263.824010</td>\n",
       "      <td>290.186913</td>\n",
       "      <td>246.810469</td>\n",
       "      <td>252.476041</td>\n",
       "      <td>322.610209</td>\n",
       "      <td>280.743361</td>\n",
       "      <td>288.407746</td>\n",
       "      <td>...</td>\n",
       "      <td>88436.529876</td>\n",
       "      <td>88332.518976</td>\n",
       "      <td>87564.244287</td>\n",
       "      <td>88660.640939</td>\n",
       "      <td>88424.979052</td>\n",
       "      <td>87185.488018</td>\n",
       "      <td>87703.940233</td>\n",
       "      <td>89191.145040</td>\n",
       "      <td>88471.467381</td>\n",
       "      <td>1.410073e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>178.319540</td>\n",
       "      <td>178.036825</td>\n",
       "      <td>182.137730</td>\n",
       "      <td>194.870821</td>\n",
       "      <td>179.637096</td>\n",
       "      <td>191.377291</td>\n",
       "      <td>178.339336</td>\n",
       "      <td>178.435581</td>\n",
       "      <td>190.275108</td>\n",
       "      <td>178.158506</td>\n",
       "      <td>...</td>\n",
       "      <td>10108.670480</td>\n",
       "      <td>10156.721862</td>\n",
       "      <td>10174.872639</td>\n",
       "      <td>10515.790588</td>\n",
       "      <td>10489.965853</td>\n",
       "      <td>10565.601127</td>\n",
       "      <td>10430.102374</td>\n",
       "      <td>10442.279774</td>\n",
       "      <td>10572.633956</td>\n",
       "      <td>5.600730e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>51444.747000</td>\n",
       "      <td>51304.021600</td>\n",
       "      <td>49409.891500</td>\n",
       "      <td>49814.045300</td>\n",
       "      <td>51660.410600</td>\n",
       "      <td>50205.352800</td>\n",
       "      <td>51141.599700</td>\n",
       "      <td>50628.552800</td>\n",
       "      <td>47273.983600</td>\n",
       "      <td>1.191378e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>117.007350</td>\n",
       "      <td>116.477950</td>\n",
       "      <td>116.269700</td>\n",
       "      <td>76.573300</td>\n",
       "      <td>123.088200</td>\n",
       "      <td>67.495700</td>\n",
       "      <td>89.112800</td>\n",
       "      <td>166.907750</td>\n",
       "      <td>103.621650</td>\n",
       "      <td>126.430400</td>\n",
       "      <td>...</td>\n",
       "      <td>81445.718650</td>\n",
       "      <td>81102.528950</td>\n",
       "      <td>80546.907150</td>\n",
       "      <td>80803.234450</td>\n",
       "      <td>80788.756250</td>\n",
       "      <td>79083.975350</td>\n",
       "      <td>79920.395700</td>\n",
       "      <td>81584.298250</td>\n",
       "      <td>80400.615700</td>\n",
       "      <td>1.371208e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>282.739600</td>\n",
       "      <td>280.673700</td>\n",
       "      <td>315.465600</td>\n",
       "      <td>247.233200</td>\n",
       "      <td>286.184900</td>\n",
       "      <td>214.922300</td>\n",
       "      <td>233.675600</td>\n",
       "      <td>356.333100</td>\n",
       "      <td>267.170400</td>\n",
       "      <td>288.203900</td>\n",
       "      <td>...</td>\n",
       "      <td>90310.572000</td>\n",
       "      <td>90083.765400</td>\n",
       "      <td>88073.786700</td>\n",
       "      <td>91008.814000</td>\n",
       "      <td>89940.028200</td>\n",
       "      <td>87663.582800</td>\n",
       "      <td>89003.815900</td>\n",
       "      <td>92114.119000</td>\n",
       "      <td>90699.133500</td>\n",
       "      <td>1.402170e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>437.293800</td>\n",
       "      <td>444.120100</td>\n",
       "      <td>455.152650</td>\n",
       "      <td>444.672800</td>\n",
       "      <td>460.145000</td>\n",
       "      <td>433.310400</td>\n",
       "      <td>408.797000</td>\n",
       "      <td>481.671100</td>\n",
       "      <td>464.014100</td>\n",
       "      <td>448.374300</td>\n",
       "      <td>...</td>\n",
       "      <td>97418.703800</td>\n",
       "      <td>97448.106150</td>\n",
       "      <td>97336.083150</td>\n",
       "      <td>97813.297150</td>\n",
       "      <td>97720.383600</td>\n",
       "      <td>97475.999350</td>\n",
       "      <td>97501.539800</td>\n",
       "      <td>98073.001950</td>\n",
       "      <td>97663.051250</td>\n",
       "      <td>1.446064e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>105790.374500</td>\n",
       "      <td>105526.299600</td>\n",
       "      <td>105420.107400</td>\n",
       "      <td>105395.279300</td>\n",
       "      <td>105539.986600</td>\n",
       "      <td>105447.760000</td>\n",
       "      <td>105301.202500</td>\n",
       "      <td>104602.318800</td>\n",
       "      <td>105390.327100</td>\n",
       "      <td>1.583052e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 X1            X2            X3            X4            X5  \\\n",
       "count  71999.000000  71999.000000  71999.000000  71999.000000  71999.000000   \n",
       "mean     281.278924    279.316030    294.136617    263.824010    290.186913   \n",
       "std      178.319540    178.036825    182.137730    194.870821    179.637096   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%      117.007350    116.477950    116.269700     76.573300    123.088200   \n",
       "50%      282.739600    280.673700    315.465600    247.233200    286.184900   \n",
       "75%      437.293800    444.120100    455.152650    444.672800    460.145000   \n",
       "max      566.000000    566.000000    566.000000    566.000000    566.000000   \n",
       "\n",
       "                 X6            X7            X8            X9           X10  \\\n",
       "count  71999.000000  71999.000000  71999.000000  71999.000000  71999.000000   \n",
       "mean     246.810469    252.476041    322.610209    280.743361    288.407746   \n",
       "std      191.377291    178.339336    178.435581    190.275108    178.158506   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%       67.495700     89.112800    166.907750    103.621650    126.430400   \n",
       "50%      214.922300    233.675600    356.333100    267.170400    288.203900   \n",
       "75%      433.310400    408.797000    481.671100    464.014100    448.374300   \n",
       "max      566.000000    566.000000    566.000000    566.000000    566.000000   \n",
       "\n",
       "       ...         Power8         Power9        Power10        Power11  \\\n",
       "count  ...   71999.000000   71999.000000   71999.000000   71999.000000   \n",
       "mean   ...   88436.529876   88332.518976   87564.244287   88660.640939   \n",
       "std    ...   10108.670480   10156.721862   10174.872639   10515.790588   \n",
       "min    ...   51444.747000   51304.021600   49409.891500   49814.045300   \n",
       "25%    ...   81445.718650   81102.528950   80546.907150   80803.234450   \n",
       "50%    ...   90310.572000   90083.765400   88073.786700   91008.814000   \n",
       "75%    ...   97418.703800   97448.106150   97336.083150   97813.297150   \n",
       "max    ...  105790.374500  105526.299600  105420.107400  105395.279300   \n",
       "\n",
       "             Power12        Power13        Power14        Power15  \\\n",
       "count   71999.000000   71999.000000   71999.000000   71999.000000   \n",
       "mean    88424.979052   87185.488018   87703.940233   89191.145040   \n",
       "std     10489.965853   10565.601127   10430.102374   10442.279774   \n",
       "min     51660.410600   50205.352800   51141.599700   50628.552800   \n",
       "25%     80788.756250   79083.975350   79920.395700   81584.298250   \n",
       "50%     89940.028200   87663.582800   89003.815900   92114.119000   \n",
       "75%     97720.383600   97475.999350   97501.539800   98073.001950   \n",
       "max    105539.986600  105447.760000  105301.202500  104602.318800   \n",
       "\n",
       "             Power16      Powerall  \n",
       "count   71999.000000  7.199900e+04  \n",
       "mean    88471.467381  1.410073e+06  \n",
       "std     10572.633956  5.600730e+04  \n",
       "min     47273.983600  1.191378e+06  \n",
       "25%     80400.615700  1.371208e+06  \n",
       "50%     90699.133500  1.402170e+06  \n",
       "75%     97663.051250  1.446064e+06  \n",
       "max    105390.327100  1.583052e+06  \n",
       "\n",
       "[8 rows x 49 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Perth DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72000 entries, 0 to 71999\n",
      "Data columns (total 49 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   X1        72000 non-null  float64\n",
      " 1   X2        72000 non-null  float64\n",
      " 2   X3        72000 non-null  float64\n",
      " 3   X4        72000 non-null  float64\n",
      " 4   X5        72000 non-null  float64\n",
      " 5   X6        72000 non-null  float64\n",
      " 6   X7        72000 non-null  float64\n",
      " 7   X8        72000 non-null  float64\n",
      " 8   X9        72000 non-null  float64\n",
      " 9   X10       72000 non-null  float64\n",
      " 10  X11       72000 non-null  float64\n",
      " 11  X12       72000 non-null  float64\n",
      " 12  X13       72000 non-null  float64\n",
      " 13  X14       72000 non-null  float64\n",
      " 14  X15       72000 non-null  float64\n",
      " 15  X16       72000 non-null  float64\n",
      " 16  Y1        72000 non-null  float64\n",
      " 17  Y2        72000 non-null  float64\n",
      " 18  Y3        72000 non-null  float64\n",
      " 19  Y4        72000 non-null  float64\n",
      " 20  Y5        72000 non-null  float64\n",
      " 21  Y6        72000 non-null  float64\n",
      " 22  Y7        72000 non-null  float64\n",
      " 23  Y8        72000 non-null  float64\n",
      " 24  Y9        72000 non-null  float64\n",
      " 25  Y10       72000 non-null  float64\n",
      " 26  Y11       72000 non-null  float64\n",
      " 27  Y12       72000 non-null  float64\n",
      " 28  Y13       72000 non-null  float64\n",
      " 29  Y14       72000 non-null  float64\n",
      " 30  Y15       72000 non-null  float64\n",
      " 31  Y16       72000 non-null  float64\n",
      " 32  Power1    72000 non-null  float64\n",
      " 33  Power2    72000 non-null  float64\n",
      " 34  Power3    72000 non-null  float64\n",
      " 35  Power4    72000 non-null  float64\n",
      " 36  Power5    72000 non-null  float64\n",
      " 37  Power6    72000 non-null  float64\n",
      " 38  Power7    72000 non-null  float64\n",
      " 39  Power8    72000 non-null  float64\n",
      " 40  Power9    72000 non-null  float64\n",
      " 41  Power10   72000 non-null  float64\n",
      " 42  Power11   72000 non-null  float64\n",
      " 43  Power12   72000 non-null  float64\n",
      " 44  Power13   72000 non-null  float64\n",
      " 45  Power14   72000 non-null  float64\n",
      " 46  Power15   72000 non-null  float64\n",
      " 47  Power16   72000 non-null  float64\n",
      " 48  Powerall  72000 non-null  float64\n",
      "dtypes: float64(49)\n",
      "memory usage: 26.9 MB\n",
      "\n",
      "--- Perth DataFrame Null Counts ---\n",
      "X1          0\n",
      "X2          0\n",
      "X3          0\n",
      "X4          0\n",
      "X5          0\n",
      "X6          0\n",
      "X7          0\n",
      "X8          0\n",
      "X9          0\n",
      "X10         0\n",
      "X11         0\n",
      "X12         0\n",
      "X13         0\n",
      "X14         0\n",
      "X15         0\n",
      "X16         0\n",
      "Y1          0\n",
      "Y2          0\n",
      "Y3          0\n",
      "Y4          0\n",
      "Y5          0\n",
      "Y6          0\n",
      "Y7          0\n",
      "Y8          0\n",
      "Y9          0\n",
      "Y10         0\n",
      "Y11         0\n",
      "Y12         0\n",
      "Y13         0\n",
      "Y14         0\n",
      "Y15         0\n",
      "Y16         0\n",
      "Power1      0\n",
      "Power2      0\n",
      "Power3      0\n",
      "Power4      0\n",
      "Power5      0\n",
      "Power6      0\n",
      "Power7      0\n",
      "Power8      0\n",
      "Power9      0\n",
      "Power10     0\n",
      "Power11     0\n",
      "Power12     0\n",
      "Power13     0\n",
      "Power14     0\n",
      "Power15     0\n",
      "Power16     0\n",
      "Powerall    0\n",
      "dtype: int64\n",
      "\n",
      "--- Perth DataFrame Duplicate Rows: 242 ---\n",
      "\n",
      "--- Perth DataFrame Descriptive Statistics ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>Power8</th>\n",
       "      <th>Power9</th>\n",
       "      <th>Power10</th>\n",
       "      <th>Power11</th>\n",
       "      <th>Power12</th>\n",
       "      <th>Power13</th>\n",
       "      <th>Power14</th>\n",
       "      <th>Power15</th>\n",
       "      <th>Power16</th>\n",
       "      <th>Powerall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>7.200000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>307.701005</td>\n",
       "      <td>263.069113</td>\n",
       "      <td>310.032429</td>\n",
       "      <td>280.009987</td>\n",
       "      <td>271.573004</td>\n",
       "      <td>271.825573</td>\n",
       "      <td>271.628205</td>\n",
       "      <td>265.366760</td>\n",
       "      <td>300.024726</td>\n",
       "      <td>265.180707</td>\n",
       "      <td>...</td>\n",
       "      <td>87172.791411</td>\n",
       "      <td>87227.400248</td>\n",
       "      <td>87479.702421</td>\n",
       "      <td>87259.608082</td>\n",
       "      <td>86416.545859</td>\n",
       "      <td>86879.938895</td>\n",
       "      <td>86110.179312</td>\n",
       "      <td>88026.263288</td>\n",
       "      <td>87450.093836</td>\n",
       "      <td>1.394475e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>186.406373</td>\n",
       "      <td>182.580380</td>\n",
       "      <td>188.616824</td>\n",
       "      <td>180.758049</td>\n",
       "      <td>183.357609</td>\n",
       "      <td>181.907878</td>\n",
       "      <td>180.591345</td>\n",
       "      <td>182.813160</td>\n",
       "      <td>179.271019</td>\n",
       "      <td>183.327812</td>\n",
       "      <td>...</td>\n",
       "      <td>10574.754639</td>\n",
       "      <td>10196.636050</td>\n",
       "      <td>10087.831693</td>\n",
       "      <td>10470.692266</td>\n",
       "      <td>10574.289769</td>\n",
       "      <td>9942.085387</td>\n",
       "      <td>10371.908246</td>\n",
       "      <td>10471.593551</td>\n",
       "      <td>10477.559853</td>\n",
       "      <td>5.225027e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>46239.804500</td>\n",
       "      <td>50114.929100</td>\n",
       "      <td>47183.047800</td>\n",
       "      <td>49291.778000</td>\n",
       "      <td>49549.990600</td>\n",
       "      <td>49465.685600</td>\n",
       "      <td>47027.625800</td>\n",
       "      <td>48982.438200</td>\n",
       "      <td>46881.573700</td>\n",
       "      <td>1.177711e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>132.556675</td>\n",
       "      <td>97.074000</td>\n",
       "      <td>126.138925</td>\n",
       "      <td>117.146725</td>\n",
       "      <td>98.519050</td>\n",
       "      <td>102.462975</td>\n",
       "      <td>105.551600</td>\n",
       "      <td>94.703375</td>\n",
       "      <td>134.872000</td>\n",
       "      <td>101.784400</td>\n",
       "      <td>...</td>\n",
       "      <td>79380.352150</td>\n",
       "      <td>79774.712375</td>\n",
       "      <td>79824.967950</td>\n",
       "      <td>79398.137825</td>\n",
       "      <td>78207.534625</td>\n",
       "      <td>80019.021325</td>\n",
       "      <td>78324.401825</td>\n",
       "      <td>80115.251700</td>\n",
       "      <td>79372.788325</td>\n",
       "      <td>1.359069e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>343.866650</td>\n",
       "      <td>243.846950</td>\n",
       "      <td>347.420450</td>\n",
       "      <td>282.056450</td>\n",
       "      <td>265.243800</td>\n",
       "      <td>267.376650</td>\n",
       "      <td>261.312400</td>\n",
       "      <td>251.311950</td>\n",
       "      <td>316.858700</td>\n",
       "      <td>253.393700</td>\n",
       "      <td>...</td>\n",
       "      <td>88613.666150</td>\n",
       "      <td>88460.835500</td>\n",
       "      <td>88768.455950</td>\n",
       "      <td>88450.972600</td>\n",
       "      <td>87221.242050</td>\n",
       "      <td>87720.772150</td>\n",
       "      <td>86479.809000</td>\n",
       "      <td>90513.687000</td>\n",
       "      <td>89574.041900</td>\n",
       "      <td>1.388878e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>477.285450</td>\n",
       "      <td>427.381725</td>\n",
       "      <td>480.393700</td>\n",
       "      <td>440.353300</td>\n",
       "      <td>441.640325</td>\n",
       "      <td>435.245675</td>\n",
       "      <td>436.602525</td>\n",
       "      <td>441.454300</td>\n",
       "      <td>461.048600</td>\n",
       "      <td>435.324425</td>\n",
       "      <td>...</td>\n",
       "      <td>96919.859150</td>\n",
       "      <td>96704.497125</td>\n",
       "      <td>96880.945750</td>\n",
       "      <td>96922.366100</td>\n",
       "      <td>96700.623700</td>\n",
       "      <td>96550.188550</td>\n",
       "      <td>96575.292350</td>\n",
       "      <td>97074.480500</td>\n",
       "      <td>96752.089350</td>\n",
       "      <td>1.426946e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>107228.154900</td>\n",
       "      <td>105509.436700</td>\n",
       "      <td>104693.116900</td>\n",
       "      <td>104978.220800</td>\n",
       "      <td>103852.565800</td>\n",
       "      <td>104757.583200</td>\n",
       "      <td>104435.890300</td>\n",
       "      <td>105785.806400</td>\n",
       "      <td>104430.644700</td>\n",
       "      <td>1.565836e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 X1            X2            X3            X4            X5  \\\n",
       "count  72000.000000  72000.000000  72000.000000  72000.000000  72000.000000   \n",
       "mean     307.701005    263.069113    310.032429    280.009987    271.573004   \n",
       "std      186.406373    182.580380    188.616824    180.758049    183.357609   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%      132.556675     97.074000    126.138925    117.146725     98.519050   \n",
       "50%      343.866650    243.846950    347.420450    282.056450    265.243800   \n",
       "75%      477.285450    427.381725    480.393700    440.353300    441.640325   \n",
       "max      566.000000    566.000000    566.000000    566.000000    566.000000   \n",
       "\n",
       "                 X6            X7            X8            X9           X10  \\\n",
       "count  72000.000000  72000.000000  72000.000000  72000.000000  72000.000000   \n",
       "mean     271.825573    271.628205    265.366760    300.024726    265.180707   \n",
       "std      181.907878    180.591345    182.813160    179.271019    183.327812   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%      102.462975    105.551600     94.703375    134.872000    101.784400   \n",
       "50%      267.376650    261.312400    251.311950    316.858700    253.393700   \n",
       "75%      435.245675    436.602525    441.454300    461.048600    435.324425   \n",
       "max      566.000000    566.000000    566.000000    566.000000    566.000000   \n",
       "\n",
       "       ...         Power8         Power9        Power10        Power11  \\\n",
       "count  ...   72000.000000   72000.000000   72000.000000   72000.000000   \n",
       "mean   ...   87172.791411   87227.400248   87479.702421   87259.608082   \n",
       "std    ...   10574.754639   10196.636050   10087.831693   10470.692266   \n",
       "min    ...   46239.804500   50114.929100   47183.047800   49291.778000   \n",
       "25%    ...   79380.352150   79774.712375   79824.967950   79398.137825   \n",
       "50%    ...   88613.666150   88460.835500   88768.455950   88450.972600   \n",
       "75%    ...   96919.859150   96704.497125   96880.945750   96922.366100   \n",
       "max    ...  107228.154900  105509.436700  104693.116900  104978.220800   \n",
       "\n",
       "             Power12        Power13        Power14        Power15  \\\n",
       "count   72000.000000   72000.000000   72000.000000   72000.000000   \n",
       "mean    86416.545859   86879.938895   86110.179312   88026.263288   \n",
       "std     10574.289769    9942.085387   10371.908246   10471.593551   \n",
       "min     49549.990600   49465.685600   47027.625800   48982.438200   \n",
       "25%     78207.534625   80019.021325   78324.401825   80115.251700   \n",
       "50%     87221.242050   87720.772150   86479.809000   90513.687000   \n",
       "75%     96700.623700   96550.188550   96575.292350   97074.480500   \n",
       "max    103852.565800  104757.583200  104435.890300  105785.806400   \n",
       "\n",
       "             Power16      Powerall  \n",
       "count   72000.000000  7.200000e+04  \n",
       "mean    87450.093836  1.394475e+06  \n",
       "std     10477.559853  5.225027e+04  \n",
       "min     46881.573700  1.177711e+06  \n",
       "25%     79372.788325  1.359069e+06  \n",
       "50%     89574.041900  1.388878e+06  \n",
       "75%     96752.089350  1.426946e+06  \n",
       "max    104430.644700  1.565836e+06  \n",
       "\n",
       "[8 rows x 49 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Sydney DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72000 entries, 0 to 71999\n",
      "Data columns (total 49 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   X1        72000 non-null  float64\n",
      " 1   X2        72000 non-null  float64\n",
      " 2   X3        72000 non-null  float64\n",
      " 3   X4        72000 non-null  float64\n",
      " 4   X5        72000 non-null  float64\n",
      " 5   X6        72000 non-null  float64\n",
      " 6   X7        72000 non-null  float64\n",
      " 7   X8        72000 non-null  float64\n",
      " 8   X9        72000 non-null  float64\n",
      " 9   X10       72000 non-null  float64\n",
      " 10  X11       72000 non-null  float64\n",
      " 11  X12       72000 non-null  float64\n",
      " 12  X13       72000 non-null  float64\n",
      " 13  X14       72000 non-null  float64\n",
      " 14  X15       72000 non-null  float64\n",
      " 15  X16       72000 non-null  float64\n",
      " 16  Y1        72000 non-null  float64\n",
      " 17  Y2        72000 non-null  float64\n",
      " 18  Y3        72000 non-null  float64\n",
      " 19  Y4        72000 non-null  float64\n",
      " 20  Y5        72000 non-null  float64\n",
      " 21  Y6        72000 non-null  float64\n",
      " 22  Y7        72000 non-null  float64\n",
      " 23  Y8        72000 non-null  float64\n",
      " 24  Y9        72000 non-null  float64\n",
      " 25  Y10       72000 non-null  float64\n",
      " 26  Y11       72000 non-null  float64\n",
      " 27  Y12       72000 non-null  float64\n",
      " 28  Y13       72000 non-null  float64\n",
      " 29  Y14       72000 non-null  float64\n",
      " 30  Y15       72000 non-null  float64\n",
      " 31  Y16       72000 non-null  float64\n",
      " 32  Power1    72000 non-null  float64\n",
      " 33  Power2    72000 non-null  float64\n",
      " 34  Power3    72000 non-null  float64\n",
      " 35  Power4    72000 non-null  float64\n",
      " 36  Power5    72000 non-null  float64\n",
      " 37  Power6    72000 non-null  float64\n",
      " 38  Power7    72000 non-null  float64\n",
      " 39  Power8    72000 non-null  float64\n",
      " 40  Power9    72000 non-null  float64\n",
      " 41  Power10   72000 non-null  float64\n",
      " 42  Power11   72000 non-null  float64\n",
      " 43  Power12   72000 non-null  float64\n",
      " 44  Power13   72000 non-null  float64\n",
      " 45  Power14   72000 non-null  float64\n",
      " 46  Power15   72000 non-null  float64\n",
      " 47  Power16   72000 non-null  float64\n",
      " 48  Powerall  72000 non-null  float64\n",
      "dtypes: float64(49)\n",
      "memory usage: 26.9 MB\n",
      "\n",
      "--- Sydney DataFrame Null Counts ---\n",
      "X1          0\n",
      "X2          0\n",
      "X3          0\n",
      "X4          0\n",
      "X5          0\n",
      "X6          0\n",
      "X7          0\n",
      "X8          0\n",
      "X9          0\n",
      "X10         0\n",
      "X11         0\n",
      "X12         0\n",
      "X13         0\n",
      "X14         0\n",
      "X15         0\n",
      "X16         0\n",
      "Y1          0\n",
      "Y2          0\n",
      "Y3          0\n",
      "Y4          0\n",
      "Y5          0\n",
      "Y6          0\n",
      "Y7          0\n",
      "Y8          0\n",
      "Y9          0\n",
      "Y10         0\n",
      "Y11         0\n",
      "Y12         0\n",
      "Y13         0\n",
      "Y14         0\n",
      "Y15         0\n",
      "Y16         0\n",
      "Power1      0\n",
      "Power2      0\n",
      "Power3      0\n",
      "Power4      0\n",
      "Power5      0\n",
      "Power6      0\n",
      "Power7      0\n",
      "Power8      0\n",
      "Power9      0\n",
      "Power10     0\n",
      "Power11     0\n",
      "Power12     0\n",
      "Power13     0\n",
      "Power14     0\n",
      "Power15     0\n",
      "Power16     0\n",
      "Powerall    0\n",
      "dtype: int64\n",
      "\n",
      "--- Sydney DataFrame Duplicate Rows: 27174 ---\n",
      "\n",
      "--- Sydney DataFrame Descriptive Statistics ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>Power8</th>\n",
       "      <th>Power9</th>\n",
       "      <th>Power10</th>\n",
       "      <th>Power11</th>\n",
       "      <th>Power12</th>\n",
       "      <th>Power13</th>\n",
       "      <th>Power14</th>\n",
       "      <th>Power15</th>\n",
       "      <th>Power16</th>\n",
       "      <th>Powerall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>7.200000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>324.130650</td>\n",
       "      <td>318.476619</td>\n",
       "      <td>248.650127</td>\n",
       "      <td>273.804973</td>\n",
       "      <td>364.120788</td>\n",
       "      <td>270.715581</td>\n",
       "      <td>268.090715</td>\n",
       "      <td>251.211585</td>\n",
       "      <td>257.469299</td>\n",
       "      <td>263.833769</td>\n",
       "      <td>...</td>\n",
       "      <td>92460.889647</td>\n",
       "      <td>92400.867081</td>\n",
       "      <td>92359.874711</td>\n",
       "      <td>93528.558491</td>\n",
       "      <td>92533.020595</td>\n",
       "      <td>92866.782188</td>\n",
       "      <td>94515.422258</td>\n",
       "      <td>92124.902168</td>\n",
       "      <td>92812.671388</td>\n",
       "      <td>1.486229e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>201.042023</td>\n",
       "      <td>200.450328</td>\n",
       "      <td>207.727345</td>\n",
       "      <td>202.496658</td>\n",
       "      <td>165.250467</td>\n",
       "      <td>221.881965</td>\n",
       "      <td>210.125347</td>\n",
       "      <td>191.412032</td>\n",
       "      <td>193.920143</td>\n",
       "      <td>203.080397</td>\n",
       "      <td>...</td>\n",
       "      <td>6351.754870</td>\n",
       "      <td>7314.166192</td>\n",
       "      <td>6617.035471</td>\n",
       "      <td>7108.750555</td>\n",
       "      <td>6631.046214</td>\n",
       "      <td>7179.076034</td>\n",
       "      <td>6828.345629</td>\n",
       "      <td>6978.090343</td>\n",
       "      <td>7128.152117</td>\n",
       "      <td>2.308364e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>67649.721100</td>\n",
       "      <td>67935.571500</td>\n",
       "      <td>67961.501700</td>\n",
       "      <td>67930.722800</td>\n",
       "      <td>68171.326100</td>\n",
       "      <td>68113.928900</td>\n",
       "      <td>68294.961800</td>\n",
       "      <td>66233.342800</td>\n",
       "      <td>64609.768500</td>\n",
       "      <td>1.361962e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>137.449825</td>\n",
       "      <td>131.146525</td>\n",
       "      <td>43.281500</td>\n",
       "      <td>63.622900</td>\n",
       "      <td>248.586900</td>\n",
       "      <td>38.906525</td>\n",
       "      <td>57.463400</td>\n",
       "      <td>79.835250</td>\n",
       "      <td>71.295925</td>\n",
       "      <td>48.945475</td>\n",
       "      <td>...</td>\n",
       "      <td>87871.425850</td>\n",
       "      <td>86957.349700</td>\n",
       "      <td>87505.072375</td>\n",
       "      <td>88246.974000</td>\n",
       "      <td>87321.966400</td>\n",
       "      <td>87602.965375</td>\n",
       "      <td>90044.975600</td>\n",
       "      <td>86835.587700</td>\n",
       "      <td>87872.906700</td>\n",
       "      <td>1.470987e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>355.491700</td>\n",
       "      <td>355.171500</td>\n",
       "      <td>201.883500</td>\n",
       "      <td>274.834600</td>\n",
       "      <td>388.372100</td>\n",
       "      <td>250.374600</td>\n",
       "      <td>233.006700</td>\n",
       "      <td>213.824750</td>\n",
       "      <td>239.241300</td>\n",
       "      <td>265.379750</td>\n",
       "      <td>...</td>\n",
       "      <td>92484.554050</td>\n",
       "      <td>91374.237650</td>\n",
       "      <td>92113.287250</td>\n",
       "      <td>93441.473900</td>\n",
       "      <td>93067.305700</td>\n",
       "      <td>91552.433900</td>\n",
       "      <td>94274.489100</td>\n",
       "      <td>91203.030000</td>\n",
       "      <td>91453.725900</td>\n",
       "      <td>1.487282e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>524.001500</td>\n",
       "      <td>518.242925</td>\n",
       "      <td>462.505600</td>\n",
       "      <td>468.289225</td>\n",
       "      <td>515.557000</td>\n",
       "      <td>511.209300</td>\n",
       "      <td>485.902425</td>\n",
       "      <td>422.665350</td>\n",
       "      <td>432.784500</td>\n",
       "      <td>457.983675</td>\n",
       "      <td>...</td>\n",
       "      <td>97638.051900</td>\n",
       "      <td>97719.189525</td>\n",
       "      <td>97502.161300</td>\n",
       "      <td>98653.893875</td>\n",
       "      <td>97534.197050</td>\n",
       "      <td>98739.563075</td>\n",
       "      <td>99668.561125</td>\n",
       "      <td>97512.515500</td>\n",
       "      <td>98362.513050</td>\n",
       "      <td>1.504180e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>110026.753900</td>\n",
       "      <td>109417.620000</td>\n",
       "      <td>109240.703000</td>\n",
       "      <td>109668.448700</td>\n",
       "      <td>109185.088800</td>\n",
       "      <td>109898.206000</td>\n",
       "      <td>109237.388800</td>\n",
       "      <td>109087.161400</td>\n",
       "      <td>109253.606800</td>\n",
       "      <td>1.536347e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 X1            X2            X3            X4            X5  \\\n",
       "count  72000.000000  72000.000000  72000.000000  72000.000000  72000.000000   \n",
       "mean     324.130650    318.476619    248.650127    273.804973    364.120788   \n",
       "std      201.042023    200.450328    207.727345    202.496658    165.250467   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%      137.449825    131.146525     43.281500     63.622900    248.586900   \n",
       "50%      355.491700    355.171500    201.883500    274.834600    388.372100   \n",
       "75%      524.001500    518.242925    462.505600    468.289225    515.557000   \n",
       "max      566.000000    566.000000    566.000000    566.000000    566.000000   \n",
       "\n",
       "                 X6            X7            X8            X9           X10  \\\n",
       "count  72000.000000  72000.000000  72000.000000  72000.000000  72000.000000   \n",
       "mean     270.715581    268.090715    251.211585    257.469299    263.833769   \n",
       "std      221.881965    210.125347    191.412032    193.920143    203.080397   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%       38.906525     57.463400     79.835250     71.295925     48.945475   \n",
       "50%      250.374600    233.006700    213.824750    239.241300    265.379750   \n",
       "75%      511.209300    485.902425    422.665350    432.784500    457.983675   \n",
       "max      566.000000    566.000000    566.000000    566.000000    566.000000   \n",
       "\n",
       "       ...         Power8         Power9        Power10        Power11  \\\n",
       "count  ...   72000.000000   72000.000000   72000.000000   72000.000000   \n",
       "mean   ...   92460.889647   92400.867081   92359.874711   93528.558491   \n",
       "std    ...    6351.754870    7314.166192    6617.035471    7108.750555   \n",
       "min    ...   67649.721100   67935.571500   67961.501700   67930.722800   \n",
       "25%    ...   87871.425850   86957.349700   87505.072375   88246.974000   \n",
       "50%    ...   92484.554050   91374.237650   92113.287250   93441.473900   \n",
       "75%    ...   97638.051900   97719.189525   97502.161300   98653.893875   \n",
       "max    ...  110026.753900  109417.620000  109240.703000  109668.448700   \n",
       "\n",
       "             Power12        Power13        Power14        Power15  \\\n",
       "count   72000.000000   72000.000000   72000.000000   72000.000000   \n",
       "mean    92533.020595   92866.782188   94515.422258   92124.902168   \n",
       "std      6631.046214    7179.076034    6828.345629    6978.090343   \n",
       "min     68171.326100   68113.928900   68294.961800   66233.342800   \n",
       "25%     87321.966400   87602.965375   90044.975600   86835.587700   \n",
       "50%     93067.305700   91552.433900   94274.489100   91203.030000   \n",
       "75%     97534.197050   98739.563075   99668.561125   97512.515500   \n",
       "max    109185.088800  109898.206000  109237.388800  109087.161400   \n",
       "\n",
       "             Power16      Powerall  \n",
       "count   72000.000000  7.200000e+04  \n",
       "mean    92812.671388  1.486229e+06  \n",
       "std      7128.152117  2.308364e+04  \n",
       "min     64609.768500  1.361962e+06  \n",
       "25%     87872.906700  1.470987e+06  \n",
       "50%     91453.725900  1.487282e+06  \n",
       "75%     98362.513050  1.504180e+06  \n",
       "max    109253.606800  1.536347e+06  \n",
       "\n",
       "[8 rows x 49 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Tasmania DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72000 entries, 0 to 71999\n",
      "Data columns (total 49 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   X1        72000 non-null  float64\n",
      " 1   X2        72000 non-null  float64\n",
      " 2   X3        72000 non-null  float64\n",
      " 3   X4        72000 non-null  float64\n",
      " 4   X5        72000 non-null  float64\n",
      " 5   X6        72000 non-null  float64\n",
      " 6   X7        72000 non-null  float64\n",
      " 7   X8        72000 non-null  float64\n",
      " 8   X9        72000 non-null  float64\n",
      " 9   X10       72000 non-null  float64\n",
      " 10  X11       72000 non-null  float64\n",
      " 11  X12       72000 non-null  float64\n",
      " 12  X13       72000 non-null  float64\n",
      " 13  X14       72000 non-null  float64\n",
      " 14  X15       72000 non-null  float64\n",
      " 15  X16       72000 non-null  float64\n",
      " 16  Y1        72000 non-null  float64\n",
      " 17  Y2        72000 non-null  float64\n",
      " 18  Y3        72000 non-null  float64\n",
      " 19  Y4        72000 non-null  float64\n",
      " 20  Y5        72000 non-null  float64\n",
      " 21  Y6        72000 non-null  float64\n",
      " 22  Y7        72000 non-null  float64\n",
      " 23  Y8        72000 non-null  float64\n",
      " 24  Y9        72000 non-null  float64\n",
      " 25  Y10       72000 non-null  float64\n",
      " 26  Y11       72000 non-null  float64\n",
      " 27  Y12       72000 non-null  float64\n",
      " 28  Y13       72000 non-null  float64\n",
      " 29  Y14       72000 non-null  float64\n",
      " 30  Y15       72000 non-null  float64\n",
      " 31  Y16       72000 non-null  float64\n",
      " 32  Power1    72000 non-null  float64\n",
      " 33  Power2    72000 non-null  float64\n",
      " 34  Power3    72000 non-null  float64\n",
      " 35  Power4    72000 non-null  float64\n",
      " 36  Power5    72000 non-null  float64\n",
      " 37  Power6    72000 non-null  float64\n",
      " 38  Power7    72000 non-null  float64\n",
      " 39  Power8    72000 non-null  float64\n",
      " 40  Power9    72000 non-null  float64\n",
      " 41  Power10   72000 non-null  float64\n",
      " 42  Power11   72000 non-null  float64\n",
      " 43  Power12   72000 non-null  float64\n",
      " 44  Power13   72000 non-null  float64\n",
      " 45  Power14   72000 non-null  float64\n",
      " 46  Power15   72000 non-null  float64\n",
      " 47  Power16   72000 non-null  float64\n",
      " 48  Powerall  72000 non-null  float64\n",
      "dtypes: float64(49)\n",
      "memory usage: 26.9 MB\n",
      "\n",
      "--- Tasmania DataFrame Null Counts ---\n",
      "X1          0\n",
      "X2          0\n",
      "X3          0\n",
      "X4          0\n",
      "X5          0\n",
      "X6          0\n",
      "X7          0\n",
      "X8          0\n",
      "X9          0\n",
      "X10         0\n",
      "X11         0\n",
      "X12         0\n",
      "X13         0\n",
      "X14         0\n",
      "X15         0\n",
      "X16         0\n",
      "Y1          0\n",
      "Y2          0\n",
      "Y3          0\n",
      "Y4          0\n",
      "Y5          0\n",
      "Y6          0\n",
      "Y7          0\n",
      "Y8          0\n",
      "Y9          0\n",
      "Y10         0\n",
      "Y11         0\n",
      "Y12         0\n",
      "Y13         0\n",
      "Y14         0\n",
      "Y15         0\n",
      "Y16         0\n",
      "Power1      0\n",
      "Power2      0\n",
      "Power3      0\n",
      "Power4      0\n",
      "Power5      0\n",
      "Power6      0\n",
      "Power7      0\n",
      "Power8      0\n",
      "Power9      0\n",
      "Power10     0\n",
      "Power11     0\n",
      "Power12     0\n",
      "Power13     0\n",
      "Power14     0\n",
      "Power15     0\n",
      "Power16     0\n",
      "Powerall    0\n",
      "dtype: int64\n",
      "\n",
      "--- Tasmania DataFrame Duplicate Rows: 0 ---\n",
      "\n",
      "--- Tasmania DataFrame Descriptive Statistics ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>Power8</th>\n",
       "      <th>Power9</th>\n",
       "      <th>Power10</th>\n",
       "      <th>Power11</th>\n",
       "      <th>Power12</th>\n",
       "      <th>Power13</th>\n",
       "      <th>Power14</th>\n",
       "      <th>Power15</th>\n",
       "      <th>Power16</th>\n",
       "      <th>Powerall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.00000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>7.200000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>300.334674</td>\n",
       "      <td>294.454049</td>\n",
       "      <td>289.066287</td>\n",
       "      <td>263.393313</td>\n",
       "      <td>307.688097</td>\n",
       "      <td>246.713427</td>\n",
       "      <td>278.580047</td>\n",
       "      <td>286.571972</td>\n",
       "      <td>276.475900</td>\n",
       "      <td>299.831906</td>\n",
       "      <td>...</td>\n",
       "      <td>232383.809106</td>\n",
       "      <td>235201.221458</td>\n",
       "      <td>239331.914935</td>\n",
       "      <td>235166.905914</td>\n",
       "      <td>234747.645220</td>\n",
       "      <td>233790.87416</td>\n",
       "      <td>235308.969514</td>\n",
       "      <td>238324.695375</td>\n",
       "      <td>236811.563240</td>\n",
       "      <td>3.760135e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>191.473699</td>\n",
       "      <td>178.334355</td>\n",
       "      <td>180.893467</td>\n",
       "      <td>181.188155</td>\n",
       "      <td>192.543076</td>\n",
       "      <td>182.996670</td>\n",
       "      <td>182.148218</td>\n",
       "      <td>169.949660</td>\n",
       "      <td>179.503873</td>\n",
       "      <td>185.270428</td>\n",
       "      <td>...</td>\n",
       "      <td>28444.373453</td>\n",
       "      <td>29019.822451</td>\n",
       "      <td>28813.403078</td>\n",
       "      <td>28734.076347</td>\n",
       "      <td>28698.375147</td>\n",
       "      <td>28354.13899</td>\n",
       "      <td>28499.868118</td>\n",
       "      <td>28694.132104</td>\n",
       "      <td>28045.641350</td>\n",
       "      <td>1.121467e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>129775.910100</td>\n",
       "      <td>138563.996300</td>\n",
       "      <td>126897.265500</td>\n",
       "      <td>133149.911400</td>\n",
       "      <td>128318.157900</td>\n",
       "      <td>131102.36320</td>\n",
       "      <td>134838.948900</td>\n",
       "      <td>135212.637500</td>\n",
       "      <td>128026.083200</td>\n",
       "      <td>3.235131e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>121.019925</td>\n",
       "      <td>137.409450</td>\n",
       "      <td>126.750025</td>\n",
       "      <td>104.923275</td>\n",
       "      <td>125.937475</td>\n",
       "      <td>75.863775</td>\n",
       "      <td>114.668750</td>\n",
       "      <td>143.879750</td>\n",
       "      <td>122.933775</td>\n",
       "      <td>132.523025</td>\n",
       "      <td>...</td>\n",
       "      <td>211195.885800</td>\n",
       "      <td>213104.082425</td>\n",
       "      <td>217772.123950</td>\n",
       "      <td>213166.245800</td>\n",
       "      <td>212845.370700</td>\n",
       "      <td>211827.84055</td>\n",
       "      <td>213768.764875</td>\n",
       "      <td>217256.762500</td>\n",
       "      <td>216084.572525</td>\n",
       "      <td>3.685857e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>316.564700</td>\n",
       "      <td>303.406700</td>\n",
       "      <td>301.635700</td>\n",
       "      <td>244.313250</td>\n",
       "      <td>336.138900</td>\n",
       "      <td>223.478250</td>\n",
       "      <td>264.086350</td>\n",
       "      <td>297.624550</td>\n",
       "      <td>261.173500</td>\n",
       "      <td>314.038200</td>\n",
       "      <td>...</td>\n",
       "      <td>233918.815250</td>\n",
       "      <td>236770.006100</td>\n",
       "      <td>245712.065000</td>\n",
       "      <td>237441.053000</td>\n",
       "      <td>237238.161500</td>\n",
       "      <td>236434.52870</td>\n",
       "      <td>237548.666400</td>\n",
       "      <td>242743.065400</td>\n",
       "      <td>240613.484050</td>\n",
       "      <td>3.755820e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>479.768225</td>\n",
       "      <td>447.732125</td>\n",
       "      <td>446.484425</td>\n",
       "      <td>427.202575</td>\n",
       "      <td>484.349675</td>\n",
       "      <td>407.305150</td>\n",
       "      <td>451.786725</td>\n",
       "      <td>420.962525</td>\n",
       "      <td>442.215300</td>\n",
       "      <td>471.073825</td>\n",
       "      <td>...</td>\n",
       "      <td>259325.638375</td>\n",
       "      <td>265307.618750</td>\n",
       "      <td>265738.635350</td>\n",
       "      <td>264954.393975</td>\n",
       "      <td>263741.435950</td>\n",
       "      <td>261343.41530</td>\n",
       "      <td>264541.756425</td>\n",
       "      <td>265748.740125</td>\n",
       "      <td>265161.642500</td>\n",
       "      <td>3.830819e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>282143.804400</td>\n",
       "      <td>279824.753000</td>\n",
       "      <td>283548.559400</td>\n",
       "      <td>281457.365400</td>\n",
       "      <td>281789.422200</td>\n",
       "      <td>281968.79520</td>\n",
       "      <td>286279.149600</td>\n",
       "      <td>282189.733800</td>\n",
       "      <td>283875.584200</td>\n",
       "      <td>4.241838e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 X1            X2            X3            X4            X5  \\\n",
       "count  72000.000000  72000.000000  72000.000000  72000.000000  72000.000000   \n",
       "mean     300.334674    294.454049    289.066287    263.393313    307.688097   \n",
       "std      191.473699    178.334355    180.893467    181.188155    192.543076   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%      121.019925    137.409450    126.750025    104.923275    125.937475   \n",
       "50%      316.564700    303.406700    301.635700    244.313250    336.138900   \n",
       "75%      479.768225    447.732125    446.484425    427.202575    484.349675   \n",
       "max      566.000000    566.000000    566.000000    566.000000    566.000000   \n",
       "\n",
       "                 X6            X7            X8            X9           X10  \\\n",
       "count  72000.000000  72000.000000  72000.000000  72000.000000  72000.000000   \n",
       "mean     246.713427    278.580047    286.571972    276.475900    299.831906   \n",
       "std      182.996670    182.148218    169.949660    179.503873    185.270428   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%       75.863775    114.668750    143.879750    122.933775    132.523025   \n",
       "50%      223.478250    264.086350    297.624550    261.173500    314.038200   \n",
       "75%      407.305150    451.786725    420.962525    442.215300    471.073825   \n",
       "max      566.000000    566.000000    566.000000    566.000000    566.000000   \n",
       "\n",
       "       ...         Power8         Power9        Power10        Power11  \\\n",
       "count  ...   72000.000000   72000.000000   72000.000000   72000.000000   \n",
       "mean   ...  232383.809106  235201.221458  239331.914935  235166.905914   \n",
       "std    ...   28444.373453   29019.822451   28813.403078   28734.076347   \n",
       "min    ...  129775.910100  138563.996300  126897.265500  133149.911400   \n",
       "25%    ...  211195.885800  213104.082425  217772.123950  213166.245800   \n",
       "50%    ...  233918.815250  236770.006100  245712.065000  237441.053000   \n",
       "75%    ...  259325.638375  265307.618750  265738.635350  264954.393975   \n",
       "max    ...  282143.804400  279824.753000  283548.559400  281457.365400   \n",
       "\n",
       "             Power12       Power13        Power14        Power15  \\\n",
       "count   72000.000000   72000.00000   72000.000000   72000.000000   \n",
       "mean   234747.645220  233790.87416  235308.969514  238324.695375   \n",
       "std     28698.375147   28354.13899   28499.868118   28694.132104   \n",
       "min    128318.157900  131102.36320  134838.948900  135212.637500   \n",
       "25%    212845.370700  211827.84055  213768.764875  217256.762500   \n",
       "50%    237238.161500  236434.52870  237548.666400  242743.065400   \n",
       "75%    263741.435950  261343.41530  264541.756425  265748.740125   \n",
       "max    281789.422200  281968.79520  286279.149600  282189.733800   \n",
       "\n",
       "             Power16      Powerall  \n",
       "count   72000.000000  7.200000e+04  \n",
       "mean   236811.563240  3.760135e+06  \n",
       "std     28045.641350  1.121467e+05  \n",
       "min    128026.083200  3.235131e+06  \n",
       "25%    216084.572525  3.685857e+06  \n",
       "50%    240613.484050  3.755820e+06  \n",
       "75%    265161.642500  3.830819e+06  \n",
       "max    283875.584200  4.241838e+06  \n",
       "\n",
       "[8 rows x 49 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check if dataset has any wrong values\n",
    "print(\"--- Adelaide DataFrame Info ---\")\n",
    "adelaide_df.info()\n",
    "print(\"\\n--- Adelaide DataFrame Null Counts ---\")\n",
    "print(adelaide_df.isnull().sum())\n",
    "print(f\"\\n--- Adelaide DataFrame Duplicate Rows: {adelaide_df.duplicated().sum()} ---\")\n",
    "print(\"\\n--- Adelaide DataFrame Descriptive Statistics ---\")\n",
    "display(adelaide_df.describe())\n",
    "\n",
    "print(\"\\n\\n--- Perth DataFrame Info ---\")\n",
    "perth_df.info()\n",
    "print(\"\\n--- Perth DataFrame Null Counts ---\")\n",
    "print(perth_df.isnull().sum())\n",
    "print(f\"\\n--- Perth DataFrame Duplicate Rows: {perth_df.duplicated().sum()} ---\")\n",
    "print(\"\\n--- Perth DataFrame Descriptive Statistics ---\")\n",
    "display(perth_df.describe())\n",
    "\n",
    "print(\"\\n\\n--- Sydney DataFrame Info ---\")\n",
    "sydney_df.info()\n",
    "print(\"\\n--- Sydney DataFrame Null Counts ---\")\n",
    "print(sydney_df.isnull().sum())\n",
    "print(f\"\\n--- Sydney DataFrame Duplicate Rows: {sydney_df.duplicated().sum()} ---\")\n",
    "print(\"\\n--- Sydney DataFrame Descriptive Statistics ---\")\n",
    "display(sydney_df.describe())\n",
    "\n",
    "print(\"\\n\\n--- Tasmania DataFrame Info ---\")\n",
    "tasmania_df.info()\n",
    "print(\"\\n--- Tasmania DataFrame Null Counts ---\")\n",
    "print(tasmania_df.isnull().sum())\n",
    "print(f\"\\n--- Tasmania DataFrame Duplicate Rows: {tasmania_df.duplicated().sum()} ---\")\n",
    "print(\"\\n--- Tasmania DataFrame Descriptive Statistics ---\")\n",
    "display(tasmania_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Z54dUW33xBnj"
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# cols 1-16 represent X coordinates\n",
    "# cols 17-32 represent Y coordinates\n",
    "# cols 33 - 48 represent power of single containers\n",
    "# col 49 represents sum of power in the farm\n",
    "# We want to calculate Powerall depending on X, Y coordinates\n",
    "\n",
    "X_cols = [f\"X{i}\" for i in range(1, 17)]\n",
    "Y_cols = [f\"Y{i}\" for i in range(1, 17)]\n",
    "powerall_col = [\"Powerall\"]\n",
    "\n",
    "def split_data(subset, powerall):\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(subset, powerall, test_size=0.4, random_state=44)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=44)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def prepare_subsets(dataframe):\n",
    "    subset = dataframe[X_cols + Y_cols]\n",
    "    powerall = dataframe[powerall_col]\n",
    "    return split_data(subset, powerall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_vs_actual(y_test, y_predict, name, r2, rmse, mape, show_plot=False):\n",
    "    \"\"\"\n",
    "    Generates a scatter plot of predicted vs. actual values.\n",
    "    \n",
    "    y_test: Actual values\n",
    "    y_predict: Predicted values\n",
    "    title: Title for the plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_test, y_predict, alpha=0.5, label='Predicted vs Actual')\n",
    "   \n",
    "    # Ideal line (y=x)\n",
    "    min_val = min(y_test.min().iloc[0], y_predict.min())\n",
    "    max_val = max(y_test.max().iloc[0], y_predict.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, label='Ideal Fit (y=x)')\n",
    "   \n",
    "    plt.xlabel(\"Actual Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.title(f\"Predicted vs Actual for {name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Display R2, RMSE, and MAPE values on the plot\n",
    "    textstr = f\"R2: {r2:.4f}\\nRMSE: {rmse:.2f}\\nMAPE: {mape:.2f}%\"\n",
    "    plt.gca().text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=10,\n",
    "                   verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "    \n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    plots_dir = 'plots/predicted_vs_actualy'\n",
    "    if not os.path.exists(plots_dir):\n",
    "        os.makedirs(plots_dir)\n",
    "   \n",
    "    filename = name.replace(' ', '_') + '.png'\n",
    "    filepath = os.path.join(plots_dir, filename)\n",
    "   \n",
    "    plt.savefig(filepath)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_residuals(y_test, y_predict, name,  r2, rmse, mape, show_plot=False):\n",
    "    \"\"\"\n",
    "    Generates a scatter plot of residuals vs. predicted values.\n",
    "       \n",
    "    y_test: Actual values\n",
    "    y_predict: Predicted values\n",
    "    title: Title for the plot\n",
    "    name: Name for saving the file\n",
    "    \"\"\"\n",
    "    residuals = y_test.values.flatten() - y_predict.flatten()\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_predict, residuals, alpha=0.5)\n",
    "        \n",
    "    # Horizontal line at y=0\n",
    "    plt.axhline(y=0, color='k', linestyle='--', lw=2)\n",
    "        \n",
    "    plt.xlabel(\"Predicted Values\")\n",
    "    plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
    "    plt.title(f\"Residuals vs Predicted Values for {name}\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Display R2, RMSE, and MAPE values on the plot\n",
    "    textstr = f\"R2: {r2:.4f}\\nRMSE: {rmse:.2f}\\nMAPE: {mape:.2f}%\"\n",
    "    plt.gca().text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=10,\n",
    "                   verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "    \n",
    "        \n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "        \n",
    "    plots_dir = 'plots/residuals'\n",
    "    if not os.path.exists(plots_dir):\n",
    "        os.makedirs(plots_dir)\n",
    "        \n",
    "    filename = name.replace(' ', '_') + '_residuals.png'\n",
    "    filepath = os.path.join(plots_dir, filename)\n",
    "        \n",
    "    plt.savefig(filepath)\n",
    "    plt.close()\n",
    "\n",
    "def plot_residuals_distribution(y_test, y_predict, name,  r2, rmse, mape, show_plot=False):\n",
    "    \"\"\"\n",
    "    Generates a histogram of the residuals to show their distribution.\n",
    "        \n",
    "    y_test: Actual values\n",
    "    y_predict: Predicted values\n",
    "    title: Title for the plot\n",
    "    name: Name for saving the file\n",
    "    \"\"\"\n",
    "    residuals = y_test.values.flatten() - y_predict.flatten()\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "        \n",
    "    plt.hist(residuals, bins=50, density=True, alpha=0.6, color='g', label='Residuals Histogram')\n",
    "        \n",
    "    # Fit a normal distribution to the data\n",
    "    mu, std = stats.norm.fit(residuals)\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = stats.norm.pdf(x, mu, std)\n",
    "    plt.plot(x, p, 'k', linewidth=2, label='Normal Distribution Fit')\n",
    "        \n",
    "    plt.axvline(x=0, color='r', linestyle='--', lw=2, label='Zero Residuals')\n",
    "\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(f\"Residuals Distribution for {name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Display R2, RMSE, and MAPE values on the plot\n",
    "    textstr = f\"R2: {r2:.4f}\\nRMSE: {rmse:.2f}\\nMAPE: {mape:.2f}%\"\n",
    "    plt.gca().text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=10,\n",
    "                   verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "        \n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "            \n",
    "    plots_dir = 'plots/residuals_distribution'\n",
    "    if not os.path.exists(plots_dir):\n",
    "        os.makedirs(plots_dir)\n",
    "            \n",
    "    filename = name.replace(' ', '_') + '_residuals_dist.png'\n",
    "    filepath = os.path.join(plots_dir, filename)\n",
    "            \n",
    "    plt.savefig(filepath)\n",
    "    plt.close()\n",
    "\n",
    "def make_plots(y_test, y_predict, name, r2, rmse, mape):\n",
    "    plot_predicted_vs_actual(y_test, y_predict, name, r2, rmse, mape)\n",
    "    plot_residuals(y_test, y_predict, name, r2, rmse, mape)\n",
    "    plot_residuals_distribution(y_test, y_predict, name, r2, rmse, mape)\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_poly_regression(dataframe, degree, name):\n",
    "    \n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = prepare_subsets(dataframe)\n",
    "\n",
    "    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train)\n",
    "    X_test_poly = poly_features.transform(X_test)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    model.fit(X_train_poly, y_train)\n",
    "    y_predict = model.predict(X_test_poly)\n",
    "    r2 = model.score(X_test_poly, y_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_predict) * 100\n",
    "    print(f\"{name}\\nScore: {r2}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"MAPE: {mape:.3f}%\\n\")\n",
    "\n",
    "    results[name] = {\n",
    "       \"score\": r2,\n",
    "       \"rmse\": rmse,\n",
    "       \"mape\": mape\n",
    "    }\n",
    "\n",
    "    make_plots(y_test, y_predict, name, r2, rmse, mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adelaide - Linear Regression\n",
      "Score: 0.1745709501418038\n",
      "RMSE: 51144.787\n",
      "MAPE: 2.906%\n",
      "\n",
      "Perth - Linear Regression\n",
      "Score: 0.14944002642095067\n",
      "RMSE: 48611.310\n",
      "MAPE: 2.751%\n",
      "\n",
      "Sydney - Linear Regression\n",
      "Score: 0.13260959255609428\n",
      "RMSE: 21606.139\n",
      "MAPE: 1.169%\n",
      "\n",
      "Tasmania - Linear Regression\n",
      "Score: 0.14371569409226093\n",
      "RMSE: 104221.734\n",
      "MAPE: 2.169%\n",
      "\n",
      "Adelaide - Polynomial Regression Degree 2\n",
      "Score: 0.8455546577463555\n",
      "RMSE: 22123.249\n",
      "MAPE: 1.245%\n",
      "\n",
      "Perth - Polynomial Regression Degree 2\n",
      "Score: 0.8185148398339247\n",
      "RMSE: 22454.605\n",
      "MAPE: 1.263%\n",
      "\n",
      "Sydney - Polynomial Regression Degree 2\n",
      "Score: 0.8247720650661557\n",
      "RMSE: 9711.169\n",
      "MAPE: 0.485%\n",
      "\n",
      "Tasmania - Polynomial Regression Degree 2\n",
      "Score: 0.7297021342418895\n",
      "RMSE: 58555.893\n",
      "MAPE: 1.217%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_poly_regression(adelaide_df, 1, \"Adelaide - Linear Regression\")\n",
    "train_poly_regression(perth_df, 1, \"Perth - Linear Regression\")\n",
    "train_poly_regression(sydney_df, 1, \"Sydney - Linear Regression\")\n",
    "train_poly_regression(tasmania_df, 1, \"Tasmania - Linear Regression\")\n",
    "\n",
    "train_poly_regression(adelaide_df, 2, \"Adelaide - Polynomial Regression Degree 2\")\n",
    "train_poly_regression(perth_df, 2, \"Perth - Polynomial Regression Degree 2\")\n",
    "train_poly_regression(sydney_df, 2, \"Sydney - Polynomial Regression Degree 2\")\n",
    "train_poly_regression(tasmania_df, 2, \"Tasmania - Polynomial Regression Degree 2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def train_xgb_model(dataframe, name):\n",
    "\n",
    "    subset = dataframe[X_cols + Y_cols]\n",
    "    powerall = dataframe[powerall_col]\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(subset, powerall, test_size=0.2, random_state=44)\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor (objective=\"reg:squarederror\", n_estimators = 1500, random_state=44, subsample= 0.8, learning_rate= 0.07)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    y_predict = xgb_model.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, y_predict)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_predict) * 100\n",
    "\n",
    "    print(f\"{name}\\nScore: {r2}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"MAPE: {mape:.3f}%\\n\")\n",
    "\n",
    "    results[name] = {\n",
    "        \"score\": r2,\n",
    "        \"rmse\": rmse,\n",
    "        \"mape\": mape\n",
    "        }\n",
    "    \n",
    "    make_plots(y_test, y_predict, name, r2, rmse, mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgb_model(adelaide_df, \"Adelaide - XGBoost Regressor\")\n",
    "train_xgb_model(perth_df, \"Perth - XGBoost Regressor\")\n",
    "train_xgb_model(sydney_df, \"Sydney - XGBoost Regressor\")\n",
    "train_xgb_model(tasmania_df, \"Tasmania - XGBoost Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "def train_lgb_model(dataframe, name):\n",
    "\n",
    "    subset = dataframe[X_cols + Y_cols]\n",
    "    powerall = dataframe[powerall_col]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(subset, powerall, test_size=0.2, random_state=44)\n",
    "\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        objective=\"regression\", \n",
    "        n_estimators=1500, \n",
    "        random_state=44, \n",
    "        subsample=0.8, \n",
    "        learning_rate=0.07\n",
    "    )\n",
    "    \n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    y_predict = lgb_model.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, y_predict)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_predict) * 100\n",
    "\n",
    "    print(f\"{name}\\nScore: {r2}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"MAPE: {mape:.3f}%\\n\")\n",
    "\n",
    "    results[name] = {\n",
    "        \"score\": r2,\n",
    "        \"rmse\": rmse,\n",
    "        \"mape\": mape\n",
    "    }\n",
    "\n",
    "    make_plots(y_test, y_predict, name, r2, rmse, mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8160\n",
      "[LightGBM] [Info] Number of data points in the train set: 57599, number of used features: 32\n",
      "[LightGBM] [Info] Start training from score 1410128.507641\n",
      "Adelaide - LightGBM Regressor\n",
      "Score: 0.9017606010861957\n",
      "RMSE: 17548.033\n",
      "MAPE: 0.935%\n",
      "\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005468 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8160\n",
      "[LightGBM] [Info] Number of data points in the train set: 57600, number of used features: 32\n",
      "[LightGBM] [Info] Start training from score 1394560.146675\n",
      "Perth - LightGBM Regressor\n",
      "Score: 0.8813326497717089\n",
      "RMSE: 17930.987\n",
      "MAPE: 0.974%\n",
      "\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8160\n",
      "[LightGBM] [Info] Number of data points in the train set: 57600, number of used features: 32\n",
      "[LightGBM] [Info] Start training from score 1486274.549599\n",
      "Sydney - LightGBM Regressor\n",
      "Score: 0.9216526280627938\n",
      "RMSE: 6466.842\n",
      "MAPE: 0.299%\n",
      "\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8160\n",
      "[LightGBM] [Info] Number of data points in the train set: 57600, number of used features: 32\n",
      "[LightGBM] [Info] Start training from score 3760217.709349\n",
      "Tasmania - LightGBM Regressor\n",
      "Score: 0.823317988582976\n",
      "RMSE: 47012.798\n",
      "MAPE: 0.960%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_lgb_model(adelaide_df, \"Adelaide - LightGBM Regressor\")\n",
    "train_lgb_model(perth_df, \"Perth - LightGBM Regressor\")\n",
    "train_lgb_model(sydney_df, \"Sydney - LightGBM Regressor\")\n",
    "train_lgb_model(tasmania_df, \"Tasmania - LightGBM Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "\n",
    "def train_catboost_model(dataframe, name):\n",
    "    X_cols = [f\"X{i}\" for i in range(1, 17)]\n",
    "    Y_cols = [f\"Y{i}\" for i in range(1, 17)]\n",
    "    powerall_col = [\"Powerall\"]\n",
    "    subset = dataframe[X_cols + Y_cols]\n",
    "    powerall = dataframe[powerall_col]\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(subset, powerall, test_size=0.4, random_state=44)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=44)\n",
    "\n",
    "    catboost_model = cb.CatBoostRegressor(\n",
    "        iterations=3000,\n",
    "        learning_rate=0.05,\n",
    "        depth=7,\n",
    "        l2_leaf_reg=3,\n",
    "        random_seed=44,\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "    \n",
    "    catboost_model.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        eval_set=(X_val, y_val),\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    y_predict = catboost_model.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, y_predict)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_predict) * 100\n",
    "\n",
    "    print(f\"{name}\\nScore: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"MAPE: {mape:.3f}%\\n\")\n",
    "\n",
    "    results[name] = {\n",
    "        \"score\": r2,\n",
    "        \"rmse\": rmse,\n",
    "        \"mape\": mape\n",
    "    }\n",
    "\n",
    "    make_plots(y_test, y_predict, name, r2, rmse, mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adelaide - CatBoost Regressor\n",
      "Score: 0.9090\n",
      "RMSE: 16978.547\n",
      "MAPE: 0.896%\n",
      "\n",
      "Perth - CatBoost Regressor\n",
      "Score: 0.8886\n",
      "RMSE: 17595.356\n",
      "MAPE: 0.949%\n",
      "\n",
      "Sydney - CatBoost Regressor\n",
      "Score: 0.9224\n",
      "RMSE: 6462.188\n",
      "MAPE: 0.297%\n",
      "\n",
      "Tasmania - CatBoost Regressor\n",
      "Score: 0.8338\n",
      "RMSE: 45916.585\n",
      "MAPE: 0.934%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_catboost_model(adelaide_df, \"Adelaide - CatBoost Regressor\")\n",
    "train_catboost_model(perth_df, \"Perth - CatBoost Regressor\")\n",
    "train_catboost_model(sydney_df, \"Sydney - CatBoost Regressor\")\n",
    "train_catboost_model(tasmania_df, \"Tasmania - CatBoost Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble_model(dataframe, name):\n",
    "    X_cols = [f\"X{i}\" for i in range(1, 17)]\n",
    "    Y_cols = [f\"Y{i}\" for i in range(1, 17)]\n",
    "    powerall_col = [\"Powerall\"]\n",
    "    subset = dataframe[X_cols + Y_cols]\n",
    "    powerall = dataframe[powerall_col]\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(subset, powerall, test_size=0.4, random_state=44)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=44)\n",
    "\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=3000, \n",
    "                                  learning_rate=0.05, max_depth=7, subsample=0.8, \n",
    "                                  random_state=44, colsample_bytree=0.8)\n",
    "    \n",
    "    lgb_model = lgb.LGBMRegressor(objective=\"regression\", n_estimators=3000, \n",
    "                                   learning_rate=0.05, max_depth=7, subsample=0.8, \n",
    "                                   random_state=44, colsample_bytree=0.8, verbose=-1)\n",
    "    \n",
    "    catboost_model = cb.CatBoostRegressor(iterations=3000, learning_rate=0.05, \n",
    "                                          depth=7, random_seed=44, verbose=False)\n",
    "\n",
    " \n",
    "    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                  callbacks=[lgb.early_stopping(stopping_rounds=100)])\n",
    "    catboost_model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)\n",
    "\n",
    "\n",
    "    pred_xgb = xgb_model.predict(X_test)\n",
    "    pred_lgb = lgb_model.predict(X_test)\n",
    "    pred_cat = catboost_model.predict(X_test)\n",
    "\n",
    "\n",
    "    y_predict = 0.36 * pred_xgb + 0.36 * pred_lgb + 0.28 * pred_cat\n",
    "\n",
    "    r2 = r2_score(y_test, y_predict)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_predict) * 100\n",
    "\n",
    "    print(f\"{name}\\nScore: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"MAPE: {mape:.3f}%\\n\")\n",
    "\n",
    "    results[name] = {\n",
    "        \"score\": r2,\n",
    "        \"rmse\": rmse,\n",
    "        \"mape\": mape\n",
    "    }\n",
    "\n",
    "    make_plots(y_test, y_predict, name, r2, rmse, mape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\tvalid_0's l2: 2.94498e+08\n",
      "Adelaide - Ensemble (XGB+LGB+Cat)\n",
      "Score: 0.9104\n",
      "RMSE: 16847.497\n",
      "MAPE: 0.885%\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2997]\tvalid_0's l2: 3.18993e+08\n",
      "Perth - Ensemble (XGB+LGB+Cat)\n",
      "Score: 0.8896\n",
      "RMSE: 17510.181\n",
      "MAPE: 0.938%\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\tvalid_0's l2: 4.29638e+07\n",
      "Sydney - Ensemble (XGB+LGB+Cat)\n",
      "Score: 0.9271\n",
      "RMSE: 6264.027\n",
      "MAPE: 0.265%\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2999]\tvalid_0's l2: 2.14339e+09\n",
      "Tasmania - Ensemble (XGB+LGB+Cat)\n",
      "Score: 0.8347\n",
      "RMSE: 45795.159\n",
      "MAPE: 0.927%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_ensemble_model(adelaide_df, \"Adelaide - Ensemble (XGB+LGB+Cat)\")\n",
    "train_ensemble_model(perth_df, \"Perth - Ensemble (XGB+LGB+Cat)\")\n",
    "train_ensemble_model(sydney_df, \"Sydney - Ensemble (XGB+LGB+Cat)\")\n",
    "train_ensemble_model(tasmania_df, \"Tasmania - Ensemble (XGB+LGB+Cat)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adelaide - Linear Regression:\n",
      "  score: 0.1745709501418038\n",
      "  rmse: 51144.786579729225\n",
      "  mape: 2.9064190651831594\n",
      "\n",
      "Perth - Linear Regression:\n",
      "  score: 0.14944002642095067\n",
      "  rmse: 48611.30996211733\n",
      "  mape: 2.7505659369864275\n",
      "\n",
      "Sydney - Linear Regression:\n",
      "  score: 0.13260959255609428\n",
      "  rmse: 21606.138651417088\n",
      "  mape: 1.1691562311047736\n",
      "\n",
      "Tasmania - Linear Regression:\n",
      "  score: 0.14371569409226093\n",
      "  rmse: 104221.73408520623\n",
      "  mape: 2.1690888425474233\n",
      "\n",
      "Adelaide - Polynomial Regression Degree 2:\n",
      "  score: 0.8455546577463555\n",
      "  rmse: 22123.24881646634\n",
      "  mape: 1.2454148094950672\n",
      "\n",
      "Perth - Polynomial Regression Degree 2:\n",
      "  score: 0.8185148398339247\n",
      "  rmse: 22454.60466406559\n",
      "  mape: 1.2633278031244077\n",
      "\n",
      "Sydney - Polynomial Regression Degree 2:\n",
      "  score: 0.8247720650661557\n",
      "  rmse: 9711.16923726056\n",
      "  mape: 0.4848479106144605\n",
      "\n",
      "Tasmania - Polynomial Regression Degree 2:\n",
      "  score: 0.7297021342418895\n",
      "  rmse: 58555.893335135246\n",
      "  mape: 1.2166544054952246\n",
      "\n",
      "Adelaide - XGBoost Regressor:\n",
      "  score: 0.9041330814361572\n",
      "  rmse: 17334.847215940496\n",
      "  mape: 0.9189312346279621\n",
      "\n",
      "Perth - XGBoost Regressor:\n",
      "  score: 0.8830437064170837\n",
      "  rmse: 17801.24490028717\n",
      "  mape: 0.9607177227735519\n",
      "\n",
      "Sydney - XGBoost Regressor:\n",
      "  score: 0.9306460618972778\n",
      "  rmse: 6084.3688251124295\n",
      "  mape: 0.2617323771119118\n",
      "\n",
      "Tasmania - XGBoost Regressor:\n",
      "  score: 0.8288140296936035\n",
      "  rmse: 46275.81312089502\n",
      "  mape: 0.9382078424096107\n",
      "\n",
      "Adelaide - LightGBM Regressor:\n",
      "  score: 0.9017606010861957\n",
      "  rmse: 17548.032904747502\n",
      "  mape: 0.9346520021264815\n",
      "\n",
      "Perth - LightGBM Regressor:\n",
      "  score: 0.8813326497717089\n",
      "  rmse: 17930.987301299425\n",
      "  mape: 0.9743237783269092\n",
      "\n",
      "Sydney - LightGBM Regressor:\n",
      "  score: 0.9216526280627938\n",
      "  rmse: 6466.8416591884\n",
      "  mape: 0.2986854195318433\n",
      "\n",
      "Tasmania - LightGBM Regressor:\n",
      "  score: 0.823317988582976\n",
      "  rmse: 47012.797537061226\n",
      "  mape: 0.9595138757921231\n",
      "\n",
      "Adelaide - CatBoost Regressor:\n",
      "  score: 0.9090342213657769\n",
      "  rmse: 16978.54682531821\n",
      "  mape: 0.8959395412074361\n",
      "\n",
      "Perth - CatBoost Regressor:\n",
      "  score: 0.8885637683225508\n",
      "  rmse: 17595.356176766225\n",
      "  mape: 0.9486649987108396\n",
      "\n",
      "Sydney - CatBoost Regressor:\n",
      "  score: 0.9224075581765095\n",
      "  rmse: 6462.187589325303\n",
      "  mape: 0.29733523894286934\n",
      "\n",
      "Tasmania - CatBoost Regressor:\n",
      "  score: 0.8337963490033644\n",
      "  rmse: 45916.58523249038\n",
      "  mape: 0.9335772292177926\n",
      "\n",
      "Adelaide - Ensemble (XGB+LGB+Cat):\n",
      "  score: 0.9104330492347429\n",
      "  rmse: 16847.497116692695\n",
      "  mape: 0.8850776886788545\n",
      "\n",
      "Perth - Ensemble (XGB+LGB+Cat):\n",
      "  score: 0.8896400326573269\n",
      "  rmse: 17510.181001981244\n",
      "  mape: 0.9381698592002601\n",
      "\n",
      "Sydney - Ensemble (XGB+LGB+Cat):\n",
      "  score: 0.9270932772254095\n",
      "  rmse: 6264.027268596699\n",
      "  mape: 0.26504224972924756\n",
      "\n",
      "Tasmania - Ensemble (XGB+LGB+Cat):\n",
      "  score: 0.8346742354029288\n",
      "  rmse: 45795.15915319635\n",
      "  mape: 0.9267477563509156\n"
     ]
    }
   ],
   "source": [
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial, X_train, y_train, X_val, y_val):\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"n_estimators\": 2000,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"random_state\": 44,\n",
    "        \"n_jobs\": -1\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBRegressor(\n",
    "        **params,\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr(\"best_iteration\", model.best_iteration)\n",
    "    \n",
    "    \n",
    "    y_predict = model.predict(X_val)\n",
    "    \n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_predict))\n",
    "    return rmse\n",
    "\n",
    "def train_xgb_with_optuna(dataframe, name, n_trials=50):\n",
    "    \n",
    "    print(f\"--- Optuna tuning for: {name} ---\")\n",
    "    \n",
    "    X_cols = [f\"X{i}\" for i in range(1, 17)]\n",
    "    Y_cols = [f\"Y{i}\" for i in range(1, 17)]\n",
    "    powerall_col = [\"Powerall\"]\n",
    "    subset = dataframe[X_cols + Y_cols]\n",
    "    powerall = dataframe[powerall_col]\n",
    "\n",
    "\n",
    "    # 60% train, 40% temp\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(subset, powerall, test_size=0.4, random_state=44)\n",
    "    # 50% of temp -> validation\n",
    "    # 50% of temp -> test\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=44)\n",
    "\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, X_train, y_train, X_val, y_val), \n",
    "        n_trials=n_trials\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- Tuning for: {name} ended ---\")\n",
    "    print(f\"The best iteration (Validation RMSE): {study.best_value:.3f}\")\n",
    "    print(\"The best hiperparameters:\")\n",
    "    print(study.best_params)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_iteration = study.best_trial.user_attrs[\"best_iteration\"]\n",
    "    \n",
    "    # Combine train (60%) and validation (20%) sets for final training\n",
    "    X_train_full = pd.concat([X_train, X_val])\n",
    "    y_train_full = pd.concat([y_train, y_val])\n",
    "    \n",
    "    final_model = xgb.XGBRegressor(\n",
    "        **best_params,\n",
    "        n_estimators=best_iteration,\n",
    "        random_state=44,\n",
    "        n_jobs=4\n",
    "    )\n",
    "    \n",
    "    final_model.fit(X_train_full, y_train_full)\n",
    "    \n",
    "    y_predict = final_model.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_predict)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_predict) * 100\n",
    "\n",
    "    print(f\"\\n--- Results for {name} ---\")\n",
    "    print(f\"Score (R2): {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"MAPE: {mape:.3f}%\\\\n\")\n",
    "    results[f\"{name}_Optuna_R2\"] = {\"score\": r2, \"rmse\": rmse, \"mape\": mape}\n",
    "    make_plots(y_test, y_predict, name + \" - Optuna Tuning\", r2, rmse, mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-25 19:24:09,179] A new study created in memory with name: no-name-5a75e1cd-3609-4a87-bc3a-4ffe153d6d07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Optuna tuning for: Adelaide - XGBoost with Optuna HPT ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-25 19:24:22,249] Trial 0 finished with value: 19910.818767695113 and parameters: {'learning_rate': 0.148943211492241, 'max_depth': 9, 'subsample': 0.9235052938044574, 'colsample_bytree': 0.9532706264009774, 'gamma': 0.09286707381119697, 'reg_alpha': 7.516854521316566e-07, 'reg_lambda': 1.1266969160904916e-07}. Best is trial 0 with value: 19910.818767695113.\n",
      "[I 2025-11-25 19:24:30,043] Trial 1 finished with value: 18096.886804088706 and parameters: {'learning_rate': 0.01521478344333008, 'max_depth': 7, 'subsample': 0.9814707026012683, 'colsample_bytree': 0.8086820149691745, 'gamma': 0.0011704439275789116, 'reg_alpha': 7.58475459698496e-08, 'reg_lambda': 0.0014393412019140916}. Best is trial 1 with value: 18096.886804088706.\n",
      "[I 2025-11-25 19:24:32,799] Trial 2 finished with value: 17880.78208580374 and parameters: {'learning_rate': 0.1259564093326749, 'max_depth': 4, 'subsample': 0.7984258683767347, 'colsample_bytree': 0.831797284417624, 'gamma': 0.011440338699832754, 'reg_alpha': 0.00046310550288307926, 'reg_lambda': 1.0226797262539709}. Best is trial 2 with value: 17880.78208580374.\n",
      "[I 2025-11-25 19:24:36,880] Trial 3 finished with value: 17486.495818202115 and parameters: {'learning_rate': 0.06255859839247228, 'max_depth': 5, 'subsample': 0.8863695432374181, 'colsample_bytree': 0.7461885472968448, 'gamma': 6.333077242892583e-08, 'reg_alpha': 7.861038567942537e-08, 'reg_lambda': 0.1080349889358814}. Best is trial 3 with value: 17486.495818202115.\n",
      "[I 2025-11-25 19:24:40,721] Trial 4 finished with value: 17790.497238694596 and parameters: {'learning_rate': 0.11584283608116171, 'max_depth': 5, 'subsample': 0.7355365497259247, 'colsample_bytree': 0.8420054945898446, 'gamma': 0.050676710781317524, 'reg_alpha': 2.979825448365293, 'reg_lambda': 6.85282659043898e-05}. Best is trial 3 with value: 17486.495818202115.\n",
      "[I 2025-11-25 19:24:51,908] Trial 5 finished with value: 17864.609931369898 and parameters: {'learning_rate': 0.04469726533917474, 'max_depth': 8, 'subsample': 0.71407954818914, 'colsample_bytree': 0.6651741852346028, 'gamma': 0.1145633604815208, 'reg_alpha': 3.701765572625945e-05, 'reg_lambda': 1.314836382430253}. Best is trial 3 with value: 17486.495818202115.\n",
      "[I 2025-11-25 19:24:56,152] Trial 6 finished with value: 17455.669107771264 and parameters: {'learning_rate': 0.06737690691221176, 'max_depth': 5, 'subsample': 0.734849480005365, 'colsample_bytree': 0.9425236109802126, 'gamma': 0.009871823231624718, 'reg_alpha': 0.041570185387941314, 'reg_lambda': 0.002644326467459488}. Best is trial 6 with value: 17455.669107771264.\n",
      "[I 2025-11-25 19:25:04,188] Trial 7 finished with value: 21120.72801775545 and parameters: {'learning_rate': 0.21486241568127937, 'max_depth': 10, 'subsample': 0.8972413084064317, 'colsample_bytree': 0.9585648155025241, 'gamma': 0.0009904632188919326, 'reg_alpha': 0.0015057196729295296, 'reg_lambda': 0.3509821584884511}. Best is trial 6 with value: 17455.669107771264.\n",
      "[I 2025-11-25 19:25:05,660] Trial 8 finished with value: 20229.81799226083 and parameters: {'learning_rate': 0.24427510095629032, 'max_depth': 7, 'subsample': 0.7046663216856959, 'colsample_bytree': 0.8502542322478774, 'gamma': 3.3722556147257933e-06, 'reg_alpha': 0.035875744947718245, 'reg_lambda': 4.205955639192588e-06}. Best is trial 6 with value: 17455.669107771264.\n",
      "[I 2025-11-25 19:25:16,513] Trial 9 finished with value: 18141.42750722776 and parameters: {'learning_rate': 0.049417922120626546, 'max_depth': 8, 'subsample': 0.6834614888736272, 'colsample_bytree': 0.6976081521508809, 'gamma': 0.000903493055306955, 'reg_alpha': 2.989702329505135e-08, 'reg_lambda': 0.021898583663905808}. Best is trial 6 with value: 17455.669107771264.\n",
      "[I 2025-11-25 19:25:19,069] Trial 10 finished with value: 21715.740282108734 and parameters: {'learning_rate': 0.016339203977699696, 'max_depth': 3, 'subsample': 0.6089508994252146, 'colsample_bytree': 0.9851713510638, 'gamma': 2.2737329432123815e-05, 'reg_alpha': 1.0552706782292216, 'reg_lambda': 0.0008934776411373949}. Best is trial 6 with value: 17455.669107771264.\n",
      "[I 2025-11-25 19:25:22,910] Trial 11 finished with value: 17645.220996065764 and parameters: {'learning_rate': 0.03269547168936022, 'max_depth': 5, 'subsample': 0.8395499875663254, 'colsample_bytree': 0.7362307032005578, 'gamma': 1.790975401473283e-08, 'reg_alpha': 6.000426329773475e-06, 'reg_lambda': 0.015517179002842897}. Best is trial 6 with value: 17455.669107771264.\n",
      "[I 2025-11-25 19:25:26,560] Trial 12 finished with value: 17566.460770456866 and parameters: {'learning_rate': 0.07701585355161039, 'max_depth': 5, 'subsample': 0.8137570983159379, 'colsample_bytree': 0.61702792725424, 'gamma': 1.7932017911777466e-08, 'reg_alpha': 0.048756515404175804, 'reg_lambda': 0.032150979607599904}. Best is trial 6 with value: 17455.669107771264.\n",
      "[I 2025-11-25 19:25:32,055] Trial 13 finished with value: 17504.09872001412 and parameters: {'learning_rate': 0.030780209602897297, 'max_depth': 6, 'subsample': 0.8741050744988298, 'colsample_bytree': 0.8898634716985078, 'gamma': 8.536378979766023e-07, 'reg_alpha': 0.004351393726957466, 'reg_lambda': 5.776458201689113e-05}. Best is trial 6 with value: 17455.669107771264.\n",
      "[I 2025-11-25 19:25:34,531] Trial 14 finished with value: 18199.50856479372 and parameters: {'learning_rate': 0.0735877490118346, 'max_depth': 3, 'subsample': 0.7700288510665922, 'colsample_bytree': 0.7446383837527104, 'gamma': 2.587417320981033e-07, 'reg_alpha': 2.9258420067696746e-05, 'reg_lambda': 9.330827280431219}. Best is trial 6 with value: 17455.669107771264.\n",
      "[I 2025-11-25 19:25:40,004] Trial 15 finished with value: 17666.23808285171 and parameters: {'learning_rate': 0.02599341875079643, 'max_depth': 6, 'subsample': 0.9675986066580541, 'colsample_bytree': 0.9026942008972099, 'gamma': 5.896499486644475e-05, 'reg_alpha': 0.13453081241452047, 'reg_lambda': 0.004347337277222497}. Best is trial 6 with value: 17455.669107771264.\n",
      "[I 2025-11-25 19:25:43,019] Trial 16 finished with value: 17818.158827443425 and parameters: {'learning_rate': 0.08764802954474024, 'max_depth': 4, 'subsample': 0.6395472872325961, 'colsample_bytree': 0.7644273876050276, 'gamma': 0.9020205144566594, 'reg_alpha': 8.463367088313478e-07, 'reg_lambda': 2.541125809904196e-06}. Best is trial 6 with value: 17455.669107771264.\n",
      "[I 2025-11-25 19:25:46,166] Trial 17 finished with value: 17761.17563676459 and parameters: {'learning_rate': 0.05774258321158555, 'max_depth': 4, 'subsample': 0.8516719316047334, 'colsample_bytree': 0.9034776659014624, 'gamma': 5.056127212398499e-06, 'reg_alpha': 0.007331801371471344, 'reg_lambda': 0.13936921818736822}. Best is trial 6 with value: 17455.669107771264.\n",
      "[I 2025-11-25 19:25:51,155] Trial 18 finished with value: 17691.219969238977 and parameters: {'learning_rate': 0.02438691877849213, 'max_depth': 6, 'subsample': 0.9293966848128388, 'colsample_bytree': 0.6852823047955882, 'gamma': 1.2688001741269686e-07, 'reg_alpha': 7.675656082634781e-05, 'reg_lambda': 0.0005938188194365624}. Best is trial 6 with value: 17455.669107771264.\n",
      "[I 2025-11-25 19:25:53,242] Trial 19 finished with value: 18426.807428309443 and parameters: {'learning_rate': 0.16531668624565365, 'max_depth': 5, 'subsample': 0.774454397261317, 'colsample_bytree': 0.7884811752775278, 'gamma': 0.004776230548570787, 'reg_alpha': 0.4676908706740716, 'reg_lambda': 6.353480768162044e-05}. Best is trial 6 with value: 17455.669107771264.\n",
      "[I 2025-11-25 19:25:56,200] Trial 20 finished with value: 17711.231238962468 and parameters: {'learning_rate': 0.09750753039133672, 'max_depth': 4, 'subsample': 0.6681416568151751, 'colsample_bytree': 0.6157980521482033, 'gamma': 0.00015627141015194128, 'reg_alpha': 2.9087310659640405e-07, 'reg_lambda': 9.260661967173277}. Best is trial 6 with value: 17455.669107771264.\n",
      "[I 2025-11-25 19:26:01,705] Trial 21 finished with value: 19153.95228144834 and parameters: {'learning_rate': 0.010296977588251396, 'max_depth': 6, 'subsample': 0.8840314274195237, 'colsample_bytree': 0.8847319390899163, 'gamma': 2.3539554700747305e-07, 'reg_alpha': 0.006078919591097365, 'reg_lambda': 5.6545013004561805e-05}. Best is trial 6 with value: 17455.669107771264.\n",
      "[I 2025-11-25 19:26:09,777] Trial 22 finished with value: 17478.97662908215 and parameters: {'learning_rate': 0.03785282502544929, 'max_depth': 7, 'subsample': 0.8619192466458616, 'colsample_bytree': 0.934213649974215, 'gamma': 1.3883064913995959e-06, 'reg_alpha': 0.002523517710740267, 'reg_lambda': 6.373633626011735e-06}. Best is trial 6 with value: 17455.669107771264.\n",
      "[I 2025-11-25 19:26:18,074] Trial 23 finished with value: 17475.550921215618 and parameters: {'learning_rate': 0.041083639734777876, 'max_depth': 7, 'subsample': 0.7467991895167053, 'colsample_bytree': 0.9489414750309683, 'gamma': 2.2979927665092113e-06, 'reg_alpha': 0.00035618887738604626, 'reg_lambda': 3.254577464881455e-08}. Best is trial 6 with value: 17455.669107771264.\n",
      "[I 2025-11-25 19:26:31,188] Trial 24 finished with value: 17993.799821049473 and parameters: {'learning_rate': 0.04376436002893084, 'max_depth': 8, 'subsample': 0.7617100956721117, 'colsample_bytree': 0.999471045214939, 'gamma': 7.991795183986691e-06, 'reg_alpha': 0.0003698688413699485, 'reg_lambda': 1.6511989429610953e-07}. Best is trial 6 with value: 17455.669107771264.\n",
      "[I 2025-11-25 19:26:39,333] Trial 25 finished with value: 17404.578707914767 and parameters: {'learning_rate': 0.0326010596981214, 'max_depth': 7, 'subsample': 0.7352120596932021, 'colsample_bytree': 0.9362611384874165, 'gamma': 0.0002163027776429706, 'reg_alpha': 0.025629137578843907, 'reg_lambda': 1.1173118424566874e-08}. Best is trial 25 with value: 17404.578707914767.\n",
      "[I 2025-11-25 19:27:01,529] Trial 26 finished with value: 18380.66897585613 and parameters: {'learning_rate': 0.020284517540921313, 'max_depth': 9, 'subsample': 0.7112844812218686, 'colsample_bytree': 0.9299851628026621, 'gamma': 9.335814037269162e-05, 'reg_alpha': 5.7910036441965, 'reg_lambda': 1.262405876792386e-08}. Best is trial 25 with value: 17404.578707914767.\n",
      "[I 2025-11-25 19:27:10,627] Trial 27 finished with value: 17621.528651056356 and parameters: {'learning_rate': 0.06384213349838785, 'max_depth': 7, 'subsample': 0.741348413294376, 'colsample_bytree': 0.9709223946790928, 'gamma': 0.0062974834596188165, 'reg_alpha': 0.06832402945930878, 'reg_lambda': 1.0998652919733014e-08}. Best is trial 25 with value: 17404.578707914767.\n",
      "[I 2025-11-25 19:27:23,301] Trial 28 finished with value: 17989.452020559158 and parameters: {'learning_rate': 0.037775202606281734, 'max_depth': 8, 'subsample': 0.8117419730713743, 'colsample_bytree': 0.8705909041939428, 'gamma': 2.1790154855346553e-05, 'reg_alpha': 0.015221947988703403, 'reg_lambda': 1.4490103028482485e-07}. Best is trial 25 with value: 17404.578707914767.\n",
      "[I 2025-11-25 19:27:43,782] Trial 29 finished with value: 18401.34951572846 and parameters: {'learning_rate': 0.01854348783640724, 'max_depth': 9, 'subsample': 0.6593383561129311, 'colsample_bytree': 0.9238461085083303, 'gamma': 0.000566201005840935, 'reg_alpha': 0.3006038817207207, 'reg_lambda': 3.849020384681649e-08}. Best is trial 25 with value: 17404.578707914767.\n",
      "[I 2025-11-25 19:28:15,791] Trial 30 finished with value: 19147.900981569754 and parameters: {'learning_rate': 0.02485690409868286, 'max_depth': 10, 'subsample': 0.7410830539933477, 'colsample_bytree': 0.9482320562450639, 'gamma': 0.031215740911771327, 'reg_alpha': 0.0006870708347307375, 'reg_lambda': 8.204515296882103e-07}. Best is trial 25 with value: 17404.578707914767.\n",
      "[I 2025-11-25 19:28:24,905] Trial 31 finished with value: 17466.827989076894 and parameters: {'learning_rate': 0.03787134418141302, 'max_depth': 7, 'subsample': 0.7958156693708067, 'colsample_bytree': 0.9403964141393799, 'gamma': 1.7130281852701635e-06, 'reg_alpha': 0.001220572837308031, 'reg_lambda': 7.782617297971585e-07}. Best is trial 25 with value: 17404.578707914767.\n",
      "[I 2025-11-25 19:28:34,369] Trial 32 finished with value: 17510.106795790824 and parameters: {'learning_rate': 0.032321838755936645, 'max_depth': 7, 'subsample': 0.7810688764189311, 'colsample_bytree': 0.9714927524192825, 'gamma': 6.187367778959463e-07, 'reg_alpha': 0.0001207668406537246, 'reg_lambda': 5.359674218156495e-07}. Best is trial 25 with value: 17404.578707914767.\n",
      "[I 2025-11-25 19:28:43,468] Trial 33 finished with value: 17605.172876174776 and parameters: {'learning_rate': 0.04621648818386801, 'max_depth': 7, 'subsample': 0.8154391297907114, 'colsample_bytree': 0.9146450714658061, 'gamma': 0.0003967646479625414, 'reg_alpha': 8.060964993867216e-06, 'reg_lambda': 5.243533382061271e-08}. Best is trial 25 with value: 17404.578707914767.\n",
      "[I 2025-11-25 19:28:49,523] Trial 34 finished with value: 17364.747737873993 and parameters: {'learning_rate': 0.05730264510691313, 'max_depth': 6, 'subsample': 0.7531847227280297, 'colsample_bytree': 0.9444802648103376, 'gamma': 2.5847623995280148e-05, 'reg_alpha': 0.001018949689077486, 'reg_lambda': 3.2658327683647606e-08}. Best is trial 34 with value: 17364.747737873993.\n",
      "[I 2025-11-25 19:28:55,230] Trial 35 finished with value: 17299.49085956 and parameters: {'learning_rate': 0.054661819857065565, 'max_depth': 6, 'subsample': 0.6930187833511031, 'colsample_bytree': 0.8629562804087731, 'gamma': 2.0401212058123916e-05, 'reg_alpha': 0.017587446925232952, 'reg_lambda': 5.795348487278384e-07}. Best is trial 35 with value: 17299.49085956.\n",
      "[I 2025-11-25 19:28:58,703] Trial 36 finished with value: 17969.698049772567 and parameters: {'learning_rate': 0.11425016296859741, 'max_depth': 6, 'subsample': 0.699190626320463, 'colsample_bytree': 0.8088234531360605, 'gamma': 1.8027168320030252e-05, 'reg_alpha': 0.01613305076605288, 'reg_lambda': 3.051971892605996e-07}. Best is trial 35 with value: 17299.49085956.\n",
      "[I 2025-11-25 19:29:03,518] Trial 37 finished with value: 17475.88967692346 and parameters: {'learning_rate': 0.05654719751407751, 'max_depth': 5, 'subsample': 0.725499960419268, 'colsample_bytree': 0.862969430352585, 'gamma': 0.002147696250454251, 'reg_alpha': 1.4839015132299833, 'reg_lambda': 5.0417502839845034e-08}. Best is trial 35 with value: 17299.49085956.\n",
      "[I 2025-11-25 19:29:06,712] Trial 38 finished with value: 18534.121613931424 and parameters: {'learning_rate': 0.14915969469494517, 'max_depth': 6, 'subsample': 0.6415148737397588, 'colsample_bytree': 0.8224474669898685, 'gamma': 0.0002021947723998688, 'reg_alpha': 0.13272444274014997, 'reg_lambda': 1.9659911441038195e-05}. Best is trial 35 with value: 17299.49085956.\n",
      "[I 2025-11-25 19:29:10,622] Trial 39 finished with value: 17511.2891587113 and parameters: {'learning_rate': 0.07271205712275874, 'max_depth': 5, 'subsample': 0.6871999232606709, 'colsample_bytree': 0.8773235410413117, 'gamma': 0.5614903710505597, 'reg_alpha': 0.013743890441281839, 'reg_lambda': 0.0002224886451952109}. Best is trial 35 with value: 17299.49085956.\n",
      "[I 2025-11-25 19:29:14,658] Trial 40 finished with value: 17622.392119119355 and parameters: {'learning_rate': 0.08992946816765751, 'max_depth': 4, 'subsample': 0.728166553561208, 'colsample_bytree': 0.8370838052682039, 'gamma': 5.341479733312413e-05, 'reg_alpha': 0.34744928908428163, 'reg_lambda': 0.002489490728116887}. Best is trial 35 with value: 17299.49085956.\n",
      "[I 2025-11-25 19:29:29,015] Trial 41 finished with value: 18038.966710984307 and parameters: {'learning_rate': 0.051874625921732045, 'max_depth': 8, 'subsample': 0.7841505480892179, 'colsample_bytree': 0.9505148791366461, 'gamma': 8.45052478938365e-06, 'reg_alpha': 0.0017473139555949744, 'reg_lambda': 1.7340180088784138e-06}. Best is trial 35 with value: 17299.49085956.\n",
      "[I 2025-11-25 19:29:35,387] Trial 42 finished with value: 17417.537828292494 and parameters: {'learning_rate': 0.061976397612048956, 'max_depth': 6, 'subsample': 0.7606162232546728, 'colsample_bytree': 0.9795666726850167, 'gamma': 0.014527696726376669, 'reg_alpha': 0.025576901203411277, 'reg_lambda': 7.9859661494016e-07}. Best is trial 35 with value: 17299.49085956.\n",
      "[I 2025-11-25 19:29:41,292] Trial 43 finished with value: 17476.67519867552 and parameters: {'learning_rate': 0.07023416816077754, 'max_depth': 6, 'subsample': 0.751427008609543, 'colsample_bytree': 0.9756827186594976, 'gamma': 0.13984402898948492, 'reg_alpha': 0.030073376272413275, 'reg_lambda': 1.2021698635336007e-07}. Best is trial 35 with value: 17299.49085956.\n",
      "[I 2025-11-25 19:29:47,097] Trial 44 finished with value: 17383.49101877986 and parameters: {'learning_rate': 0.06160019046171987, 'max_depth': 6, 'subsample': 0.7159582718782327, 'colsample_bytree': 0.9979090455367094, 'gamma': 0.02522963601812801, 'reg_alpha': 0.10308654521308884, 'reg_lambda': 3.197716956702803e-07}. Best is trial 35 with value: 17299.49085956.\n",
      "[I 2025-11-25 19:29:54,006] Trial 45 finished with value: 17347.93128877331 and parameters: {'learning_rate': 0.05191580184818339, 'max_depth': 6, 'subsample': 0.6875964254733299, 'colsample_bytree': 0.994442160407614, 'gamma': 0.024399638758561215, 'reg_alpha': 0.8036302657217717, 'reg_lambda': 3.0952153252235715e-07}. Best is trial 35 with value: 17299.49085956.\n",
      "[I 2025-11-25 19:29:58,352] Trial 46 finished with value: 17391.565771948193 and parameters: {'learning_rate': 0.050904634185742585, 'max_depth': 5, 'subsample': 0.6852079495573338, 'colsample_bytree': 0.9947751471561949, 'gamma': 0.2192583743498516, 'reg_alpha': 1.7053323807987206, 'reg_lambda': 1.0234512039306225e-05}. Best is trial 35 with value: 17299.49085956.\n",
      "[I 2025-11-25 19:30:03,068] Trial 47 finished with value: 17454.3720597448 and parameters: {'learning_rate': 0.050834685211553675, 'max_depth': 5, 'subsample': 0.6163698703576768, 'colsample_bytree': 0.9634034776703428, 'gamma': 0.23686464598886853, 'reg_alpha': 2.778615623122895, 'reg_lambda': 7.2480986383273795e-06}. Best is trial 35 with value: 17299.49085956.\n",
      "[I 2025-11-25 19:30:05,782] Trial 48 finished with value: 17786.490154046693 and parameters: {'learning_rate': 0.11250066979447693, 'max_depth': 5, 'subsample': 0.6737958199609767, 'colsample_bytree': 0.9950054905523147, 'gamma': 0.04929105469983021, 'reg_alpha': 6.208254369703313, 'reg_lambda': 1.3560140294740983e-05}. Best is trial 35 with value: 17299.49085956.\n",
      "[I 2025-11-25 19:30:11,629] Trial 49 finished with value: 17471.632780023738 and parameters: {'learning_rate': 0.08250370329923991, 'max_depth': 6, 'subsample': 0.6517442140525556, 'colsample_bytree': 0.9947529518159872, 'gamma': 0.002028219365747079, 'reg_alpha': 1.1243156532270164, 'reg_lambda': 2.1037438180148957e-06}. Best is trial 35 with value: 17299.49085956.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tuning for: Adelaide - XGBoost with Optuna HPT ended ---\n",
      "The best iteration (Validation RMSE): 17299.491\n",
      "The best hiperparameters:\n",
      "{'learning_rate': 0.054661819857065565, 'max_depth': 6, 'subsample': 0.6930187833511031, 'colsample_bytree': 0.8629562804087731, 'gamma': 2.0401212058123916e-05, 'reg_alpha': 0.017587446925232952, 'reg_lambda': 5.795348487278384e-07}\n",
      "\n",
      "--- Results for Adelaide - XGBoost with Optuna HPT ---\n",
      "Score (R2): 0.9065\n",
      "RMSE: 17210.135\n",
      "MAPE: 0.910%\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-25 19:30:15,795] A new study created in memory with name: no-name-5296a051-94e6-419f-8d38-907a66f73f97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Optuna tuning for: Perth - XGBoost with Optuna HPT ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-25 19:30:18,536] Trial 0 finished with value: 18486.82038642665 and parameters: {'learning_rate': 0.15495368417212518, 'max_depth': 3, 'subsample': 0.7721313792260174, 'colsample_bytree': 0.8622195992891464, 'gamma': 0.037910207213409736, 'reg_alpha': 0.03580079198531317, 'reg_lambda': 5.070606457799454e-07}. Best is trial 0 with value: 18486.82038642665.\n",
      "[I 2025-11-25 19:30:21,679] Trial 1 finished with value: 18165.12350632387 and parameters: {'learning_rate': 0.05231488259597966, 'max_depth': 4, 'subsample': 0.826854127409946, 'colsample_bytree': 0.7399301744972612, 'gamma': 0.6983729647098038, 'reg_alpha': 0.015768876537286647, 'reg_lambda': 4.7222843340641765}. Best is trial 1 with value: 18165.12350632387.\n",
      "[I 2025-11-25 19:30:26,602] Trial 2 finished with value: 18963.363414753196 and parameters: {'learning_rate': 0.12843953439647274, 'max_depth': 7, 'subsample': 0.9786960771655163, 'colsample_bytree': 0.7076956272529076, 'gamma': 0.1775775897829326, 'reg_alpha': 2.154895000645241e-05, 'reg_lambda': 0.021035963384790107}. Best is trial 1 with value: 18165.12350632387.\n",
      "[I 2025-11-25 19:30:27,405] Trial 3 finished with value: 20396.48126515944 and parameters: {'learning_rate': 0.2625434085137618, 'max_depth': 6, 'subsample': 0.7119865284588615, 'colsample_bytree': 0.9080753397606915, 'gamma': 0.004503214459113338, 'reg_alpha': 0.0038407511581887155, 'reg_lambda': 0.051865006803073176}. Best is trial 1 with value: 18165.12350632387.\n",
      "[I 2025-11-25 19:30:30,088] Trial 4 finished with value: 19069.21623979339 and parameters: {'learning_rate': 0.04637971729038817, 'max_depth': 3, 'subsample': 0.8596068419211578, 'colsample_bytree': 0.8260286923616034, 'gamma': 0.00015497761029604289, 'reg_alpha': 0.08567300554549137, 'reg_lambda': 1.4146125094634165e-05}. Best is trial 1 with value: 18165.12350632387.\n",
      "[I 2025-11-25 19:30:32,618] Trial 5 finished with value: 24184.755529051767 and parameters: {'learning_rate': 0.01093689140142971, 'max_depth': 3, 'subsample': 0.7299052891267162, 'colsample_bytree': 0.7783275290840153, 'gamma': 0.04535701276791176, 'reg_alpha': 1.6926521809404744e-07, 'reg_lambda': 4.117359822075495}. Best is trial 1 with value: 18165.12350632387.\n",
      "[I 2025-11-25 19:30:33,744] Trial 6 finished with value: 21006.23069472484 and parameters: {'learning_rate': 0.2765968457637812, 'max_depth': 7, 'subsample': 0.9296511246410306, 'colsample_bytree': 0.6659489450466362, 'gamma': 0.0021717173744556237, 'reg_alpha': 0.21488730197449263, 'reg_lambda': 0.009530577593462602}. Best is trial 1 with value: 18165.12350632387.\n",
      "[I 2025-11-25 19:30:37,894] Trial 7 finished with value: 18062.05303945263 and parameters: {'learning_rate': 0.07497356133821306, 'max_depth': 5, 'subsample': 0.9908579087376946, 'colsample_bytree': 0.9373080813669632, 'gamma': 0.01609023513583706, 'reg_alpha': 2.9410827349757187, 'reg_lambda': 0.2362247471609856}. Best is trial 7 with value: 18062.05303945263.\n",
      "[I 2025-11-25 19:30:59,772] Trial 8 finished with value: 20478.37884208611 and parameters: {'learning_rate': 0.08819138294039608, 'max_depth': 10, 'subsample': 0.6078782609652436, 'colsample_bytree': 0.8657779255446872, 'gamma': 6.352751213862948e-07, 'reg_alpha': 0.012097815503147587, 'reg_lambda': 0.003001056446007129}. Best is trial 7 with value: 18062.05303945263.\n",
      "[I 2025-11-25 19:31:17,496] Trial 9 finished with value: 20953.233258855304 and parameters: {'learning_rate': 0.12611784464116949, 'max_depth': 10, 'subsample': 0.7004065945137093, 'colsample_bytree': 0.8820088951333054, 'gamma': 0.3610770915850594, 'reg_alpha': 1.6879195455943875e-07, 'reg_lambda': 0.06852715806945173}. Best is trial 7 with value: 18062.05303945263.\n",
      "[I 2025-11-25 19:31:21,780] Trial 10 finished with value: 18529.296155008156 and parameters: {'learning_rate': 0.026966731596204208, 'max_depth': 5, 'subsample': 0.9960343016064633, 'colsample_bytree': 0.9944005869454506, 'gamma': 4.820269407817397e-07, 'reg_alpha': 7.661267765527628, 'reg_lambda': 3.814791144440302e-05}. Best is trial 7 with value: 18062.05303945263.\n",
      "[I 2025-11-25 19:31:25,727] Trial 11 finished with value: 17909.41205065091 and parameters: {'learning_rate': 0.0477422260949746, 'max_depth': 5, 'subsample': 0.8614407633970356, 'colsample_bytree': 0.7372917045739753, 'gamma': 0.8190641689194975, 'reg_alpha': 4.115715922566576, 'reg_lambda': 2.2092147190921834}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:31:29,513] Trial 12 finished with value: 18440.421687152386 and parameters: {'learning_rate': 0.024962781550960184, 'max_depth': 5, 'subsample': 0.8943435591499671, 'colsample_bytree': 0.6178726229726607, 'gamma': 2.5012498120321558e-05, 'reg_alpha': 6.64401280742747, 'reg_lambda': 1.1755586013677948}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:31:35,334] Trial 13 finished with value: 18125.4773178529 and parameters: {'learning_rate': 0.03151294047586918, 'max_depth': 6, 'subsample': 0.9380945541082304, 'colsample_bytree': 0.9871839439275144, 'gamma': 0.0027087159489911955, 'reg_alpha': 7.0645864528965e-05, 'reg_lambda': 0.3954613217211475}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:31:41,400] Trial 14 finished with value: 18988.110806502053 and parameters: {'learning_rate': 0.07424781004525813, 'max_depth': 8, 'subsample': 0.8685958112319339, 'colsample_bytree': 0.9474907906547209, 'gamma': 1.0807906501430491e-08, 'reg_alpha': 0.803789023392671, 'reg_lambda': 1.903684039461657e-08}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:31:45,452] Trial 15 finished with value: 19265.02821176237 and parameters: {'learning_rate': 0.01560112193359338, 'max_depth': 5, 'subsample': 0.9322947046349952, 'colsample_bytree': 0.7945315590506842, 'gamma': 0.018584947787290862, 'reg_alpha': 1.0205417917362924, 'reg_lambda': 0.0009160939882225161}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:31:56,813] Trial 16 finished with value: 18519.93347720234 and parameters: {'learning_rate': 0.04219408684175701, 'max_depth': 8, 'subsample': 0.8100988966870221, 'colsample_bytree': 0.734643840170059, 'gamma': 4.547091989478438e-05, 'reg_alpha': 0.0007205180364876405, 'reg_lambda': 0.26228477504645376}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:31:59,952] Trial 17 finished with value: 18135.27303351124 and parameters: {'learning_rate': 0.0787939209977066, 'max_depth': 4, 'subsample': 0.6241184039276744, 'colsample_bytree': 0.6724603161530721, 'gamma': 0.9746490516528062, 'reg_alpha': 7.461436928021638e-06, 'reg_lambda': 9.88830362084857}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:32:03,206] Trial 18 finished with value: 18113.022055968464 and parameters: {'learning_rate': 0.06703122994158908, 'max_depth': 4, 'subsample': 0.778134634111196, 'colsample_bytree': 0.9268428350242665, 'gamma': 0.00039470878533946557, 'reg_alpha': 1.0145449060947482, 'reg_lambda': 6.326079650445984e-05}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:32:16,086] Trial 19 finished with value: 18550.97797961067 and parameters: {'learning_rate': 0.01698807772118513, 'max_depth': 8, 'subsample': 0.9725179669927739, 'colsample_bytree': 0.8311006950161337, 'gamma': 0.022486886985720992, 'reg_alpha': 0.0013568485502917383, 'reg_lambda': 0.40302105045253994}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:32:18,674] Trial 20 finished with value: 19285.681320606745 and parameters: {'learning_rate': 0.17327737257885015, 'max_depth': 6, 'subsample': 0.9032239876004758, 'colsample_bytree': 0.617907382682308, 'gamma': 0.14068579239825402, 'reg_alpha': 2.231936909393097e-06, 'reg_lambda': 0.000319168674089586}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:32:22,542] Trial 21 finished with value: 18142.611939850336 and parameters: {'learning_rate': 0.06328307202947804, 'max_depth': 4, 'subsample': 0.7544792287137968, 'colsample_bytree': 0.9326434011332408, 'gamma': 0.0003787996758284117, 'reg_alpha': 1.0208781964422016, 'reg_lambda': 2.3733440847372446e-05}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:32:25,593] Trial 22 finished with value: 18327.120886816894 and parameters: {'learning_rate': 0.10385153446957257, 'max_depth': 5, 'subsample': 0.8452397219762, 'colsample_bytree': 0.9584657411407079, 'gamma': 0.0008973033327662835, 'reg_alpha': 4.126201371391701, 'reg_lambda': 3.0127885440703306e-06}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:32:29,236] Trial 23 finished with value: 18402.947589992207 and parameters: {'learning_rate': 0.03731348500046185, 'max_depth': 4, 'subsample': 0.6721848069894127, 'colsample_bytree': 0.9180380899818933, 'gamma': 8.307482730619338e-06, 'reg_alpha': 0.2783695373608875, 'reg_lambda': 0.001728053027391691}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:32:33,446] Trial 24 finished with value: 17942.008806151 and parameters: {'learning_rate': 0.06189573538438071, 'max_depth': 5, 'subsample': 0.7747046302210838, 'colsample_bytree': 0.7612021771200209, 'gamma': 0.006724388576979429, 'reg_alpha': 1.8036189520312291, 'reg_lambda': 8.756992546182153e-05}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:32:39,218] Trial 25 finished with value: 17941.5477593211 and parameters: {'learning_rate': 0.05712737079297689, 'max_depth': 6, 'subsample': 0.7909647736569545, 'colsample_bytree': 0.75576868181031, 'gamma': 0.014135977267247584, 'reg_alpha': 0.061816135212312454, 'reg_lambda': 1.0509620839551215e-06}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:32:48,174] Trial 26 finished with value: 18087.178221049297 and parameters: {'learning_rate': 0.034082115776018766, 'max_depth': 7, 'subsample': 0.8239213012393103, 'colsample_bytree': 0.7608580069499816, 'gamma': 0.006490848796791061, 'reg_alpha': 0.09821931057850174, 'reg_lambda': 3.728985257808023e-07}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:32:54,028] Trial 27 finished with value: 17957.746851985634 and parameters: {'learning_rate': 0.054209285380429094, 'max_depth': 6, 'subsample': 0.7903443342770761, 'colsample_bytree': 0.6923838581393469, 'gamma': 0.12277259244976045, 'reg_alpha': 0.005690202023066287, 'reg_lambda': 2.431909347345355e-08}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:33:12,599] Trial 28 finished with value: 18917.660743337165 and parameters: {'learning_rate': 0.025013182772450817, 'max_depth': 9, 'subsample': 0.7485081758490364, 'colsample_bytree': 0.7299415440648049, 'gamma': 0.06634303681831843, 'reg_alpha': 0.00019705162319229774, 'reg_lambda': 2.6547061033701216e-06}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:33:16,223] Trial 29 finished with value: 18519.561117909896 and parameters: {'learning_rate': 0.10122855534555862, 'max_depth': 6, 'subsample': 0.6619030900620514, 'colsample_bytree': 0.8032874686774366, 'gamma': 0.013957857401476523, 'reg_alpha': 0.03851047561724925, 'reg_lambda': 1.5721501180490985e-07}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:33:19,323] Trial 30 finished with value: 19805.612133938197 and parameters: {'learning_rate': 0.183630437904598, 'max_depth': 7, 'subsample': 0.7976518568900995, 'colsample_bytree': 0.7637636549074059, 'gamma': 0.001432643914167648, 'reg_alpha': 0.33247354258928846, 'reg_lambda': 0.0001770802150144613}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:33:24,810] Trial 31 finished with value: 17913.933794675027 and parameters: {'learning_rate': 0.05455739441922319, 'max_depth': 6, 'subsample': 0.7852670637015785, 'colsample_bytree': 0.6915785400014149, 'gamma': 0.14464233144087044, 'reg_alpha': 0.00426759521592938, 'reg_lambda': 1.0866536366620789e-08}. Best is trial 11 with value: 17909.41205065091.\n",
      "[I 2025-11-25 19:33:28,800] Trial 32 finished with value: 17888.563497385698 and parameters: {'learning_rate': 0.05109053278139158, 'max_depth': 5, 'subsample': 0.7653519534009848, 'colsample_bytree': 0.6443244391294978, 'gamma': 0.3220133788530105, 'reg_alpha': 0.051871943122158724, 'reg_lambda': 8.033128348365935e-08}. Best is trial 32 with value: 17888.563497385698.\n",
      "[I 2025-11-25 19:33:34,058] Trial 33 finished with value: 17864.31258123301 and parameters: {'learning_rate': 0.04794776879314128, 'max_depth': 6, 'subsample': 0.8307000165452059, 'colsample_bytree': 0.6419059960394298, 'gamma': 0.4225238610698154, 'reg_alpha': 0.02704750548153372, 'reg_lambda': 1.0364658854221607e-07}. Best is trial 33 with value: 17864.31258123301.\n",
      "[I 2025-11-25 19:33:38,242] Trial 34 finished with value: 17976.1210498817 and parameters: {'learning_rate': 0.043304128486428346, 'max_depth': 5, 'subsample': 0.849573420759504, 'colsample_bytree': 0.6467776084449895, 'gamma': 0.28768281773875853, 'reg_alpha': 0.01229678444820747, 'reg_lambda': 1.1374194517951674e-08}. Best is trial 33 with value: 17864.31258123301.\n",
      "[I 2025-11-25 19:33:45,932] Trial 35 finished with value: 18206.402390368065 and parameters: {'learning_rate': 0.046392771480821236, 'max_depth': 7, 'subsample': 0.8270418867636745, 'colsample_bytree': 0.7085717803531986, 'gamma': 0.9167469010869369, 'reg_alpha': 0.0020565253722419027, 'reg_lambda': 8.142848373341392e-08}. Best is trial 33 with value: 17864.31258123301.\n",
      "[I 2025-11-25 19:33:48,561] Trial 36 finished with value: 19094.115533325967 and parameters: {'learning_rate': 0.048920528941098246, 'max_depth': 3, 'subsample': 0.7502535499199203, 'colsample_bytree': 0.6551107772788318, 'gamma': 0.37784197071415354, 'reg_alpha': 0.000405177973242144, 'reg_lambda': 8.519611123004139e-08}. Best is trial 33 with value: 17864.31258123301.\n",
      "[I 2025-11-25 19:33:53,816] Trial 37 finished with value: 18019.28922016626 and parameters: {'learning_rate': 0.029916667224295857, 'max_depth': 6, 'subsample': 0.8803147432001643, 'colsample_bytree': 0.6001392776347612, 'gamma': 0.06304914686333717, 'reg_alpha': 0.020090939538266916, 'reg_lambda': 5.368834087316193e-06}. Best is trial 33 with value: 17864.31258123301.\n",
      "[I 2025-11-25 19:33:57,984] Trial 38 finished with value: 18813.607841134566 and parameters: {'learning_rate': 0.01866459295662938, 'max_depth': 5, 'subsample': 0.715025523649831, 'colsample_bytree': 0.6926756765709027, 'gamma': 0.17413331335013577, 'reg_alpha': 0.006414368891369352, 'reg_lambda': 4.866646745398369e-07}. Best is trial 33 with value: 17864.31258123301.\n",
      "[I 2025-11-25 19:34:03,822] Trial 39 finished with value: 17880.538694345872 and parameters: {'learning_rate': 0.039385906419515775, 'max_depth': 6, 'subsample': 0.8289888146268698, 'colsample_bytree': 0.6393817210953624, 'gamma': 0.48807084232685244, 'reg_alpha': 3.9599988252981524e-08, 'reg_lambda': 7.520389799822906e-08}. Best is trial 33 with value: 17864.31258123301.\n",
      "[W 2025-11-25 19:34:04,767] Trial 40 failed with parameters: {'learning_rate': 0.020670329162850474, 'max_depth': 7, 'subsample': 0.8353246270144584, 'colsample_bytree': 0.6482774702413276, 'gamma': 0.48060941209199376, 'reg_alpha': 8.234625654702657e-07, 'reg_lambda': 9.125118485151385e-08} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/sn/3fp31rj12dnfk_ggs5y20bd80000gn/T/ipykernel_3395/2169338795.py\", line 66, in <lambda>\n",
      "    lambda trial: objective(trial, X_train, y_train, X_val, y_val),\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sn/3fp31rj12dnfk_ggs5y20bd80000gn/T/ipykernel_3395/2169338795.py\", line 30, in objective\n",
      "    model.fit(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py\", line 774, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1368, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py\", line 774, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/xgboost/training.py\", line 200, in train\n",
      "    if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/xgboost/callback.py\", line 269, in after_iteration\n",
      "    ret = any(c.after_iteration(model, epoch, self.history) for c in self.callbacks)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-11-25 19:34:04,771] Trial 40 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_xgb_with_optuna(adelaide_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdelaide - XGBoost with Optuna HPT\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m train_xgb_with_optuna(perth_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerth - XGBoost with Optuna HPT\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      3\u001b[0m train_xgb_with_optuna(sydney_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSydney - XGBoost with Optuna HPT\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      4\u001b[0m train_xgb_with_optuna(tasmania_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTasmania - XGBoost with Optuna HPT\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "Cell \u001b[0;32mIn[42], line 65\u001b[0m, in \u001b[0;36mtrain_xgb_with_optuna\u001b[0;34m(dataframe, name, n_trials)\u001b[0m\n\u001b[1;32m     61\u001b[0m X_val, X_test, y_val, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_temp, y_temp, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m44\u001b[39m)\n\u001b[1;32m     64\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m trial: objective(trial, X_train, y_train, X_val, y_val), \n\u001b[1;32m     67\u001b[0m     n_trials\u001b[38;5;241m=\u001b[39mn_trials\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Tuning for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ended ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe best iteration (Validation RMSE): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/optuna/study/study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 490\u001b[0m     _optimize(\n\u001b[1;32m    491\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    492\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    493\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[1;32m    494\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    495\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    496\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[1;32m    497\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    498\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[1;32m    499\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[1;32m    500\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/optuna/study/_optimize.py:67\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 67\u001b[0m         _optimize_sequential(\n\u001b[1;32m     68\u001b[0m             study,\n\u001b[1;32m     69\u001b[0m             func,\n\u001b[1;32m     70\u001b[0m             n_trials,\n\u001b[1;32m     71\u001b[0m             timeout,\n\u001b[1;32m     72\u001b[0m             catch,\n\u001b[1;32m     73\u001b[0m             callbacks,\n\u001b[1;32m     74\u001b[0m             gc_after_trial,\n\u001b[1;32m     75\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     76\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     77\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m     78\u001b[0m         )\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/optuna/study/_optimize.py:164\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     frozen_trial_id \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/optuna/study/_optimize.py:262\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    258\u001b[0m     updated_state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    261\u001b[0m ):\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\u001b[38;5;241m.\u001b[39m_trial_id\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/optuna/study/_optimize.py:205\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[42], line 66\u001b[0m, in \u001b[0;36mtrain_xgb_with_optuna.<locals>.<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     61\u001b[0m X_val, X_test, y_val, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_temp, y_temp, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m44\u001b[39m)\n\u001b[1;32m     64\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m trial: objective(trial, X_train, y_train, X_val, y_val), \n\u001b[1;32m     67\u001b[0m     n_trials\u001b[38;5;241m=\u001b[39mn_trials\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Tuning for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ended ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe best iteration (Validation RMSE): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[42], line 30\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m     10\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreg:squarederror\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_metric\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     23\u001b[0m }\n\u001b[1;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m     27\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[1;32m     28\u001b[0m )\n\u001b[0;32m---> 30\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     31\u001b[0m     X_train, \n\u001b[1;32m     32\u001b[0m     y_train, \n\u001b[1;32m     33\u001b[0m     eval_set\u001b[38;5;241m=\u001b[39m[(X_val, y_val)],\n\u001b[1;32m     34\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     37\u001b[0m trial\u001b[38;5;241m.\u001b[39mset_user_attr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m.\u001b[39mbest_iteration)\n\u001b[1;32m     40\u001b[0m y_predict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:774\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    773\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 774\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/sklearn.py:1368\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1366\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m   1369\u001b[0m     params,\n\u001b[1;32m   1370\u001b[0m     train_dmatrix,\n\u001b[1;32m   1371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_boosting_rounds(),\n\u001b[1;32m   1372\u001b[0m     evals\u001b[38;5;241m=\u001b[39mevals,\n\u001b[1;32m   1373\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_rounds,\n\u001b[1;32m   1374\u001b[0m     evals_result\u001b[38;5;241m=\u001b[39mevals_result,\n\u001b[1;32m   1375\u001b[0m     obj\u001b[38;5;241m=\u001b[39mobj,\n\u001b[1;32m   1376\u001b[0m     custom_metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m   1377\u001b[0m     verbose_eval\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   1378\u001b[0m     xgb_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1379\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[1;32m   1380\u001b[0m )\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:774\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    773\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 774\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/training.py:200\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, iteration\u001b[38;5;241m=\u001b[39mi, fobj\u001b[38;5;241m=\u001b[39mobj)\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    203\u001b[0m bst \u001b[38;5;241m=\u001b[39m cb_container\u001b[38;5;241m.\u001b[39mafter_training(bst)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/callback.py:269\u001b[0m, in \u001b[0;36mCallbackContainer.after_iteration\u001b[0;34m(self, model, epoch, dtrain, evals)\u001b[0m\n\u001b[1;32m    267\u001b[0m     metric_score \u001b[38;5;241m=\u001b[39m _parse_eval_str(score)\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_history(metric_score, epoch)\n\u001b[0;32m--> 269\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(c\u001b[38;5;241m.\u001b[39mafter_iteration(model, epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_xgb_with_optuna(adelaide_df, \"Adelaide - XGBoost with Optuna HPT\", n_trials=50)\n",
    "train_xgb_with_optuna(perth_df, \"Perth - XGBoost with Optuna HPT\", n_trials=50)\n",
    "train_xgb_with_optuna(sydney_df, \"Sydney - XGBoost with Optuna HPT\", n_trials=50)\n",
    "train_xgb_with_optuna(tasmania_df, \"Tasmania - XGBoost with Optuna HPT\", n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgb(trial, X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"regression_l2\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"n_estimators\": 2000,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 150),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"random_state\": 44,\n",
    "        \"n_jobs\": -1,\n",
    "        \"verbose\": -1\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"rmse\",\n",
    "        callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr(\"best_iteration\", model.best_iteration_)\n",
    "    \n",
    "    y_predict = model.predict(X_val)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_predict))\n",
    "    return rmse\n",
    "\n",
    "def train_lgb_with_optuna(dataframe, name, n_trials=50):\n",
    "    \n",
    "    print(f\"--- Optuna tuning for: {name} ---\")\n",
    "    \n",
    "    X_cols = [f\"X{i}\" for i in range(1, 17)]\n",
    "    Y_cols = [f\"Y{i}\" for i in range(1, 17)]\n",
    "    powerall_col = [\"Powerall\"]\n",
    "    subset = dataframe[X_cols + Y_cols]\n",
    "    powerall = dataframe[powerall_col]\n",
    "\n",
    "    # 60% train, 40% temp\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(subset, powerall, test_size=0.4, random_state=44)\n",
    "    # 50% of temp -> validation\n",
    "    # 50% of temp -> test\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=44)\n",
    "\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(\n",
    "        lambda trial: objective_lgb(trial, X_train, y_train, X_val, y_val), \n",
    "        n_trials=n_trials\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- Tuning for: {name} ended ---\")\n",
    "    print(f\"The best iteration (Validation RMSE): {study.best_value:.3f}\")\n",
    "    print(\"The best hiperparameters:\")\n",
    "    print(study.best_params)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_iteration = study.best_trial.user_attrs[\"best_iteration\"]\n",
    "    \n",
    "    # Combine train (60%) and validation (20%) sets for final training\n",
    "    X_train_full = pd.concat([X_train, X_val])\n",
    "    y_train_full = pd.concat([y_train, y_val])\n",
    "    \n",
    "    final_model = lgb.LGBMRegressor(\n",
    "        **best_params,\n",
    "        n_estimators=best_iteration,\n",
    "        random_state=44,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    final_model.fit(X_train_full, y_train_full)\n",
    "    \n",
    "    y_predict = final_model.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_predict)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_predict) * 100\n",
    "\n",
    "    print(f\"\\n--- Results for {name} ---\")\n",
    "    print(f\"Score (R2): {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"MAPE: {mape:.3f}%\\\\n\")\n",
    "    results[f\"{name}_Optuna_LGB\"] = {\"score\": r2, \"rmse\": rmse, \"mape\": mape}\n",
    "    make_plots(y_test, y_predict, name + \" - Optuna Tuning\", r2, rmse, mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lgb_with_optuna(adelaide_df, \"Adelaide - LightGBM with Optuna HPT\", n_trials=50)\n",
    "train_lgb_with_optuna(perth_df, \"Perth - LightGBM with Optuna HPT\", n_trials=50)\n",
    "train_lgb_with_optuna(sydney_df, \"Sydney - LightGBM with Optuna HPT\", n_trials=50)\n",
    "train_lgb_with_optuna(tasmania_df, \"Tasmania - LightGBM with Optuna HPT\", n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "\n",
    "def objective_cb(trial, X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"RMSE\",\n",
    "        \"eval_metric\": \"RMSE\",\n",
    "        \"iterations\": 2000,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 3, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-8, 10.0, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 1e-8, 10.0, log=True),\n",
    "        \"random_seed\": 44,\n",
    "        \"thread_count\": -1,\n",
    "        \"verbose\": False\n",
    "    }\n",
    "\n",
    "    model = cb.CatBoostRegressor(\n",
    "        **params,\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=(X_val, y_val),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr(\"best_iteration\", model.get_best_iteration())\n",
    "    \n",
    "    y_predict = model.predict(X_val)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_predict))\n",
    "    return rmse\n",
    "\n",
    "def train_cb_with_optuna(dataframe, name, n_trials=50):\n",
    "    \n",
    "    print(f\"--- Optuna tuning for: {name} ---\")\n",
    "    \n",
    "    X_cols = [f\"X{i}\" for i in range(1, 17)]\n",
    "    Y_cols = [f\"Y{i}\" for i in range(1, 17)]\n",
    "    powerall_col = [\"Powerall\"]\n",
    "    subset = dataframe[X_cols + Y_cols]\n",
    "    powerall = dataframe[powerall_col]\n",
    "\n",
    "    # 60% train, 40% temp\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(subset, powerall, test_size=0.4, random_state=44)\n",
    "    # 50% of temp -> validation\n",
    "    # 50% of temp -> test\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=44)\n",
    "\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(\n",
    "        lambda trial: objective_cb(trial, X_train, y_train, X_val, y_val), \n",
    "        n_trials=n_trials\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- Tuning for: {name} ended ---\")\n",
    "    print(f\"The best iteration (Validation RMSE): {study.best_value:.3f}\")\n",
    "    print(\"The best hiperparameters:\")\n",
    "    print(study.best_params)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_iteration = study.best_trial.user_attrs[\"best_iteration\"]\n",
    "    \n",
    "    # Combine train (60%) and validation (20%) sets for final training\n",
    "    X_train_full = pd.concat([X_train, X_val])\n",
    "    y_train_full = pd.concat([y_train, y_val])\n",
    "    \n",
    "    final_model = cb.CatBoostRegressor(\n",
    "        **best_params,\n",
    "        iterations=best_iteration,\n",
    "        random_seed=44,\n",
    "        thread_count=-1,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    final_model.fit(X_train_full, y_train_full, verbose=False)\n",
    "    \n",
    "    y_predict = final_model.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_predict)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_predict) * 100\n",
    "\n",
    "    print(f\"\\n--- Results for {name} ---\")\n",
    "    print(f\"Score (R2): {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"MAPE: {mape:.3f}%\\\\n\")\n",
    "    results[f\"{name}_Optuna_CB\"] = {\"score\": r2, \"rmse\": rmse, \"mape\": mape}\n",
    "    make_plots(y_test, y_predict, name + \" - Optuna Tuning\", r2, rmse, mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cb_with_optuna(adelaide_df, \"Adelaide - CatBoost with Optuna HPT\", n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cb_with_optuna(perth_df, \"Perth - CatBoost with Optuna HPT\", n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cb_with_optuna(sydney_df, \"Sydney - CatBoost with Optuna HPT\", n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_cb_with_optuna(tasmania_df, \"Tasmania - CatBoost with Optuna HPT\", n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
